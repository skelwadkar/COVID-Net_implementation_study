{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import sklearn\n",
    "import glob\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "#import matplotlib as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hu_to_uint8(hu_images, window_width, window_center):\n",
    "    \"\"\"Converts HU images to uint8 images\"\"\"\n",
    "    images = (hu_images.astype(np.float) - window_center + window_width/2)/window_width\n",
    "    uint8_images = np.uint8(255.0*np.clip(images, 0.0, 1.0))\n",
    "    return uint8_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_uint8(data, window_width=1500, window_center=-600):\n",
    "    \"\"\"Converts non-uint8 data to uint8 and applies window level to HU data\"\"\"\n",
    "    if data.dtype != np.uint8:\n",
    "        if data.ptp() > 255:\n",
    "            # Assume HU\n",
    "            data = hu_to_uint8(data, window_width, window_center)\n",
    "        data = data.astype(np.uint8)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contours(binary_image):\n",
    "    \"\"\"Helper function for finding contours\"\"\"\n",
    "    return cv2.findContours(binary_image, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_contour(binary_image):\n",
    "    \"\"\"Helper function to get body contour\"\"\"\n",
    "    contours = find_contours(binary_image)\n",
    "    areas = [cv2.contourArea(cnt) for cnt in contours]\n",
    "    body_idx = np.argmax(areas)\n",
    "    return contours[body_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_body_crop(image, scale=1.0):\n",
    "    \"\"\"Roughly crop an image to the body region\"\"\"\n",
    "    # Create initial binary image\n",
    "    filt_image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    thresh = cv2.threshold(filt_image[filt_image > 0], 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[0]\n",
    "    bin_image = np.uint8(filt_image > thresh)\n",
    "    erode_kernel = np.ones((7, 7), dtype=np.uint8)\n",
    "    bin_image = cv2.erode(bin_image, erode_kernel)\n",
    "\n",
    "    # Find body contour\n",
    "    body_cont = body_contour(bin_image).squeeze()\n",
    "\n",
    "    # Get bbox\n",
    "    xmin = body_cont[:, 0].min()\n",
    "    xmax = body_cont[:, 0].max() + 1\n",
    "    ymin = body_cont[:, 1].min()\n",
    "    ymax = body_cont[:, 1].max() + 1\n",
    "\n",
    "    # Scale to final bbox\n",
    "    if scale > 0 and scale != 1.0:\n",
    "        center = ((xmax + xmin)/2, (ymin + ymax)/2)\n",
    "        width = scale*(xmax - xmin + 1)\n",
    "        height = scale*(ymax - ymin + 1)\n",
    "        xmin = int(center[0] - width/2)\n",
    "        xmax = int(center[0] + width/2)\n",
    "        ymin = int(center[1] - height/2)\n",
    "        ymax = int(center[1] + height/2)\n",
    "\n",
    "    return image[ymin:ymax, xmin:xmax], (xmin, ymin, xmax, ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_ext_file_iter(directory, extensions):\n",
    "    \"\"\"Creates a multi-extension file iterator\"\"\"\n",
    "    patterns = ['*.' + ext.lower() for ext in extensions]\n",
    "    return itertools.chain.from_iterable(\n",
    "        glob.iglob(os.path.join(directory, pat)) for pat in patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = ('png', 'jpg', 'jpeg', 'tif')\n",
    "CLASS_MAP = {'Normal': 0, 'CP': 1, 'NCP': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lesion_files(lesion_file, exclude_list, root_dir):\n",
    "    \"\"\"Reads the lesion file to identify relevant\n",
    "    slices and returns their paths\"\"\"\n",
    "    files = []\n",
    "    with open(lesion_file, 'r') as f:\n",
    "        f.readline()\n",
    "        for line in f.readlines():\n",
    "            cls, pid = line.split('/')[:2]\n",
    "            if pid not in exclude_list:\n",
    "                files.append(os.path.join(root_dir, line.strip('\\n'))) ###*********###\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(lesion_file, unzip_file, exclude_file, root_dir):\n",
    "    \"\"\"Gets image file paths according to given lists\"\"\"\n",
    "    excluded_pids = get_excluded_pids(exclude_file)\n",
    "    files = get_lesion_files(lesion_file, excluded_pids['CP'] + excluded_pids['NCP'], root_dir)\n",
    "    with open(unzip_file, 'r') as f:\n",
    "        reader = list(csv.DictReader(f, delimiter=',', quotechar='|'))\n",
    "        for row in reader:\n",
    "            if row['label'] == 'Normal':\n",
    "                pid = row['patient_id']\n",
    "                sid = row['scan_id']\n",
    "                if pid not in excluded_pids['Normal']:\n",
    "                    files += get_source_paths(root_dir, 'Normal', pid, sid)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_paths(root_dir, cls, pid, sid):\n",
    "    \"\"\"Helper function to construct source paths\"\"\"\n",
    "    exam_dir = os.path.join(root_dir, cls, pid, sid)\n",
    "    return list(multi_ext_file_iter(exam_dir, IMG_EXTENSIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output_path(output_dir, source_path):\n",
    "    \"\"\"Helper function to construct output paths\"\"\"\n",
    "    source_path = source_path.replace('\\\\', '/')\n",
    "    parts = source_path.split('/')[-4:]\n",
    "    output_path = os.path.join(output_dir, '_'.join(parts))\n",
    "    output_path = os.path.splitext(output_path)[0] + '.png'\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imread_gray(path):\n",
    "    \"\"\"Reads images in grayscale\"\"\"\n",
    "    image = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    if image.ndim > 2:\n",
    "        image = image[:, :, 0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_excluded_pids(exclude_file):\n",
    "    \"\"\"Reads the exclusion list and returns a\n",
    "    dict of lists of excluded patients\"\"\"\n",
    "    exclude_pids = {\n",
    "        'NCP': [],\n",
    "        'CP': [],\n",
    "        'Normal': []\n",
    "    }\n",
    "    with open(exclude_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            cls, pid = line.strip('\\n').split()\n",
    "            exclude_pids[cls].append(pid)\n",
    "    return exclude_pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'C:/Users/sonal/OneDrive/Desktop/Covid19_project/CovidNet/COVIDNet-CT-master'\n",
    "output_dir = 'C:/Users/sonal/OneDrive/Desktop/Covid19_project/CovidNet/COVIDNet-CT-master/output'\n",
    "lesion_file = 'C:/Users/sonal/OneDrive/Desktop/Covid19_project/CovidNet/COVIDNet-CT-master/lesions_slices.csv'\n",
    "unzip_file = 'C:/Users/sonal/OneDrive/Desktop/Covid19_project/CovidNet/COVIDNet-CT-master/unzip_filenames.csv'\n",
    "exclude_file = 'C:/Users/sonal/OneDrive/Desktop/Covid19_project/CovidNet/COVIDNet-CT-master/exclude_list.txt'\n",
    "image_files = get_files(lesion_file, unzip_file, exclude_file, root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to new files as PNGs\n",
    "for imf in image_files:\n",
    "    image = imread_gray(imf)\n",
    "    output_path = make_output_path(output_dir, imf)\n",
    "    cv2.imwrite(output_path, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotation(image, max_degrees, bbox=None, prob=0.5):\n",
    "    \"\"\"Applies random rotation to image and bbox\"\"\"\n",
    "    def _rotation(image, bbox):\n",
    "        # Get random angle\n",
    "        degrees = tf.random.uniform([], minval=-max_degrees, maxval=max_degrees, dtype=tf.float32)\n",
    "        radians = degrees * math.pi / 180.\n",
    "        if bbox is not None:\n",
    "            # Get offset from image center\n",
    "            image_shape = tf.cast(tf.shape(image), tf.float32)\n",
    "            image_height, image_width = image_shape[0], image_shape[1]\n",
    "            bbox = tf.cast(bbox, tf.float32)\n",
    "            center_x = image_width / 2.\n",
    "            center_y = image_height / 2.\n",
    "            bbox_center_x = (bbox[0] + bbox[2]) / 2.\n",
    "            bbox_center_y = (bbox[1] + bbox[3]) / 2.\n",
    "            trans_x = center_x - bbox_center_x\n",
    "            trans_y = center_y - bbox_center_y\n",
    "\n",
    "            # Apply rotation\n",
    "            image = _translate_image(image, trans_x, trans_y)\n",
    "            bbox = _translate_bbox(bbox, image_height, image_width, trans_x, trans_y)\n",
    "            image = tf.contrib.image.rotate(image, radians, interpolation='BILINEAR')\n",
    "            bbox = _rotate_bbox(bbox, image_height, image_width, radians)\n",
    "            image = _translate_image(image, -trans_x, -trans_y)\n",
    "            bbox = _translate_bbox(bbox, image_height, image_width, -trans_x, -trans_y)\n",
    "            bbox = tf.cast(bbox, tf.int32)\n",
    "\n",
    "            return image, bbox\n",
    "        return tf.contrib.image.rotate(image, radians, interpolation='BILINEAR')\n",
    "\n",
    "    retval = image if bbox is None else (image, bbox)\n",
    "    return tf.cond(_should_apply(prob), lambda: _rotation(image, bbox), lambda: retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_bbox_jitter(bbox, image_height, image_width, max_fraction, prob=0.5):\n",
    "    \"\"\"Jitters bbox coordinates by +/- jitter_fraction of the width/height\"\"\"\n",
    "    def _bbox_jitter(bbox):\n",
    "        bbox = tf.cast(bbox, tf.float32)\n",
    "        width_jitter = max_fraction*(bbox[2] - bbox[0])\n",
    "        height_jitter = max_fraction*(bbox[3] - bbox[1])\n",
    "        xmin = bbox[0] + tf.random.uniform([], minval=-width_jitter, maxval=width_jitter, dtype=tf.float32)\n",
    "        ymin = bbox[1] + tf.random.uniform([], minval=-height_jitter, maxval=height_jitter, dtype=tf.float32)\n",
    "        xmax = bbox[2] + tf.random.uniform([], minval=-width_jitter, maxval=width_jitter, dtype=tf.float32)\n",
    "        ymax = bbox[3] + tf.random.uniform([], minval=-height_jitter, maxval=height_jitter, dtype=tf.float32)\n",
    "        xmin, ymin, xmax, ymax = _clip_bbox(xmin, ymin, xmax, ymax, image_height, image_width)\n",
    "        bbox = tf.cast(tf.stack([xmin, ymin, xmax, ymax]), tf.int32)\n",
    "        return bbox\n",
    "\n",
    "    return tf.cond(_should_apply(prob), lambda: _bbox_jitter(bbox), lambda: bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shift_and_scale(image, max_shift, max_scale_change, prob=0.5):\n",
    "    \"\"\"Applies random shift and scale to pixel values\"\"\"\n",
    "    def _shift_and_scale(image):\n",
    "        shift = tf.cast(tf.random.uniform([], minval=-max_shift, maxval=max_shift, dtype=tf.int32), tf.float32)\n",
    "        scale = tf.random.uniform([], minval=(1. - max_scale_change),\n",
    "                                  maxval=(1. + max_scale_change), dtype=tf.float32)\n",
    "        image = scale*(tf.cast(image, tf.float32) + shift)\n",
    "        image = tf.cast(tf.clip_by_value(image, 0., 255.), tf.uint8)\n",
    "        return image\n",
    "\n",
    "    return tf.cond(_should_apply(prob), lambda: _shift_and_scale(image), lambda: image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shear(image, max_lambda, bbox=None, prob=0.5):\n",
    "    \"\"\"Applies shear in either the x or y direction\"\"\"\n",
    "    shear_lambda = tf.random.uniform([], minval=-max_lambda, maxval=max_lambda, dtype=tf.float32)\n",
    "    image_shape = tf.cast(tf.shape(image), tf.float32)\n",
    "    image_height, image_width = image_shape[0], image_shape[1]\n",
    "\n",
    "    def _shear_x(image, bbox):\n",
    "        image = _shear_x_image(image, shear_lambda)\n",
    "        if bbox is not None:\n",
    "            bbox = _shear_bbox(bbox, image_height, image_width, shear_lambda, horizontal=True)\n",
    "            bbox = tf.cast(bbox, tf.int32)\n",
    "            return image, bbox\n",
    "        return image\n",
    "\n",
    "    def _shear_y(image, bbox):\n",
    "        image = _shear_y_image(image, shear_lambda)\n",
    "        if bbox is not None:\n",
    "            bbox = _shear_bbox(bbox, image_height, image_width, shear_lambda, horizontal=False)\n",
    "            bbox = tf.cast(bbox, tf.int32)\n",
    "            return image, bbox\n",
    "        return image\n",
    "\n",
    "    def _shear(image, bbox):\n",
    "        return tf.cond(_should_apply(0.5), lambda: _shear_x(image, bbox), lambda: _shear_y(image, bbox))\n",
    "\n",
    "    retval = image if bbox is None else (image, bbox)\n",
    "    return tf.cond(_should_apply(prob), lambda: _shear(image, bbox), lambda: retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _translate_image(image, delta_x, delta_y):\n",
    "    \"\"\"Translate an image\"\"\"\n",
    "    return tf.contrib.image.translate(image, [delta_x, delta_y], interpolation='BILINEAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _translate_bbox(bbox, image_height, image_width, delta_x, delta_y):\n",
    "    \"\"\"Translate an bbox, ensuring coordinates lie in the image\"\"\"\n",
    "    bbox = bbox + tf.stack([delta_x, delta_y, delta_x, delta_y])\n",
    "    xmin, ymin, xmax, ymax = _clip_bbox(bbox[0], bbox[1], bbox[2], bbox[3], image_height, image_width)\n",
    "    bbox = tf.stack([xmin, ymin, xmax, ymax])\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rotate_bbox(bbox, image_height, image_width, radians):\n",
    "    \"\"\"Rotates the bbox by the given angle\"\"\"\n",
    "    # Shift bbox to origin\n",
    "    xmin, ymin, xmax, ymax = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "    center_x = (xmin + xmax) / 2.\n",
    "    center_y = (ymin + ymax) / 2.\n",
    "    xmin = xmin - center_x\n",
    "    xmax = xmax - center_x\n",
    "    ymin = ymin - center_y\n",
    "    ymax = ymax - center_y\n",
    "\n",
    "    # Rotate bbox coordinates\n",
    "    radians = -radians  # negate direction since y-axis is flipped\n",
    "    coords = tf.stack([[xmin, ymin], [xmax, ymin], [xmin, ymax], [xmax, ymax]])\n",
    "    coords = tf.transpose(tf.cast(coords, tf.float32))\n",
    "    rotation_matrix = tf.stack(\n",
    "        [[tf.cos(radians), -tf.sin(radians)],\n",
    "         [tf.sin(radians), tf.cos(radians)]])\n",
    "    new_coords = tf.matmul(rotation_matrix, coords)\n",
    "\n",
    "    # Find new bbox coordinates and clip to image size\n",
    "    xmin = tf.reduce_min(new_coords[0, :]) + center_x\n",
    "    ymin = tf.reduce_min(new_coords[1, :]) + center_y\n",
    "    xmax = tf.reduce_max(new_coords[0, :]) + center_x\n",
    "    ymax = tf.reduce_max(new_coords[1, :]) + center_y\n",
    "    xmin, ymin, xmax, ymax = _clip_bbox(xmin, ymin, xmax, ymax, image_height, image_width)\n",
    "    bbox = tf.stack([xmin, ymin, xmax, ymax])\n",
    "\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shear_x_image(image, shear_lambda):\n",
    "    \"\"\"Shear image in x-direction\"\"\"\n",
    "    tform = tf.stack([1., shear_lambda, 0., 0., 1., 0., 0., 0.])\n",
    "    image = tf.contrib.image.transform(\n",
    "        image, tform, interpolation='BILINEAR')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shear_y_image(image, shear_lambda):\n",
    "    \"\"\"Shear image in y-direction\"\"\"\n",
    "    tform = tf.stack([1., 0., 0., shear_lambda, 1., 0., 0., 0.])\n",
    "    image = tf.contrib.image.transform(\n",
    "        image, tform, interpolation='BILINEAR')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shear_bbox(bbox, image_height, image_width, shear_lambda, horizontal=True):\n",
    "    \"\"\"Shear bbox in x- or y-direction\"\"\"\n",
    "    # Shear bbox coordinates\n",
    "    xmin, ymin, xmax, ymax = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "    coords = tf.stack([[xmin, ymin], [xmax, ymin], [xmin, ymax], [xmax, ymax]])\n",
    "    coords = tf.transpose(tf.cast(coords, tf.float32))\n",
    "    if horizontal:\n",
    "        shear_matrix = tf.stack(\n",
    "            [[1., -shear_lambda],\n",
    "             [0., 1.]])\n",
    "    else:\n",
    "        shear_matrix = tf.stack(\n",
    "            [[1., 0.],\n",
    "             [-shear_lambda, 1.]])\n",
    "    new_coords = tf.matmul(shear_matrix, coords)\n",
    "\n",
    "    # Find new bbox coordinates and clip to image size\n",
    "    xmin = tf.reduce_min(new_coords[0, :])\n",
    "    ymin = tf.reduce_min(new_coords[1, :])\n",
    "    xmax = tf.reduce_max(new_coords[0, :])\n",
    "    ymax = tf.reduce_max(new_coords[1, :])\n",
    "    xmin, ymin, xmax, ymax = _clip_bbox(xmin, ymin, xmax, ymax, image_height, image_width)\n",
    "    bbox = tf.stack([xmin, ymin, xmax, ymax])\n",
    "\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clip_bbox(xmin, ymin, xmax, ymax, image_height, image_width):\n",
    "    \"\"\"Clip bbox to valid image coordinates\"\"\"\n",
    "    xmin = tf.clip_by_value(xmin, 0, image_width)\n",
    "    ymin = tf.clip_by_value(ymin, 0, image_height)\n",
    "    xmax = tf.clip_by_value(xmax, 0, image_width)\n",
    "    ymax = tf.clip_by_value(ymax, 0, image_height)\n",
    "    return xmin, ymin, xmax, ymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _should_apply(prob):\n",
    "    \"\"\"Helper function to create bool tensor with probability\"\"\"\n",
    "    return tf.cast(tf.floor(tf.random_uniform([], dtype=tf.float32) + prob), tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset - COVIDxCTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COVIDxCTDataset:\n",
    "    \"\"\"COVIDx-CT dataset class, which handles construction of train/validation datasets\"\"\"\n",
    "    def __init__(self, data_dir, image_height=512, image_width=512, max_bbox_jitter=0.025, max_rotation=10,\n",
    "                 max_shear=0.15, max_pixel_shift=10, max_pixel_scale_change=0.2, shuffle_buffer=1000):\n",
    "        # General parameters\n",
    "        self.data_dir = data_dir\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "        # Augmentation parameters\n",
    "        self.max_bbox_jitter = max_bbox_jitter\n",
    "        self.max_rotation = max_rotation\n",
    "        self.max_shear = max_shear\n",
    "        self.max_pixel_shift = max_pixel_shift\n",
    "        self.max_pixel_scale_change = max_pixel_scale_change\n",
    "\n",
    "    def train_dataset(self, train_split_file='train.txt', batch_size=1):\n",
    "        \"\"\"Returns training dataset\"\"\"\n",
    "        return self._make_dataset(train_split_file, batch_size, True)\n",
    "\n",
    "    def validation_dataset(self, val_split_file='val.txt', batch_size=1):\n",
    "        \"\"\"Returns validation dataset (also used for testing)\"\"\"\n",
    "        return self._make_dataset(val_split_file, batch_size, False)\n",
    "\n",
    "    def _make_dataset(self, split_file, batch_size, is_training, balanced=True):\n",
    "        \"\"\"Creates COVIDX-CT dataset for train or val split\"\"\"\n",
    "        files, classes, bboxes = self._get_files(split_file)\n",
    "        count = len(files)\n",
    "\n",
    "        # Create balanced dataset if required\n",
    "        if is_training and balanced:\n",
    "            files = np.asarray(files)\n",
    "            classes = np.asarray(classes, dtype=np.int32)\n",
    "            bboxes = np.asarray(bboxes, dtype=np.int32)\n",
    "            class_nums = np.unique(classes)\n",
    "            class_wise_datasets = []\n",
    "            for cls in class_nums:\n",
    "                indices = np.where(classes == cls)[0]\n",
    "                class_wise_datasets.append(\n",
    "                    tf.data.Dataset.from_tensor_slices((files[indices], classes[indices], bboxes[indices])))\n",
    "            class_weights = [1.0 / len(class_nums) for _ in class_nums]\n",
    "            dataset = tf.data.experimental.sample_from_datasets(\n",
    "                class_wise_datasets, class_weights)\n",
    "        else:\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((files, classes, bboxes))\n",
    "\n",
    "        # Shuffle and repeat in training\n",
    "        if is_training:\n",
    "            dataset = dataset.shuffle(buffer_size=self.shuffle_buffer)\n",
    "            dataset = dataset.repeat()\n",
    "\n",
    "        # Create and apply map function\n",
    "        load_and_process = self._get_load_and_process_fn(is_training)\n",
    "        dataset = dataset.map(load_and_process)\n",
    "\n",
    "        # Batch data\n",
    "        dataset = dataset.batch(batch_size)\n",
    "\n",
    "        return dataset, count, batch_size\n",
    "\n",
    "    def _get_load_and_process_fn(self, is_training):\n",
    "        \"\"\"Creates map function for TF dataset\"\"\"\n",
    "        def load_and_process(path, label, bbox):\n",
    "            # Load image\n",
    "            image = tf.image.decode_png(tf.io.read_file(path), channels=1)\n",
    "\n",
    "            # Apply augmentations and/or crop to bbox\n",
    "            if is_training:\n",
    "                image, bbox = self._augment_image_and_bbox(image, bbox)\n",
    "            else:\n",
    "                image = tf.image.crop_to_bounding_box(image, bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0])\n",
    "\n",
    "            # Stack to 3-channel, scale to [0, 1] and resize\n",
    "            image = tf.image.grayscale_to_rgb(image)\n",
    "            image = tf.cast(image, tf.float32)\n",
    "            image = image / 255.0\n",
    "            image = tf.image.resize(image, [self.image_height, self.image_width])\n",
    "            label = tf.cast(label, dtype=tf.int32)\n",
    "\n",
    "            return {'image': image, 'label': label}\n",
    "\n",
    "        return load_and_process\n",
    "\n",
    "    def _augment_image_and_bbox(self, image, bbox):\n",
    "        \"\"\"Apply augmentations to image and bbox\"\"\"\n",
    "        image_shape = tf.cast(tf.shape(image), tf.float32)\n",
    "        image_height, image_width = image_shape[0], image_shape[1]\n",
    "        bbox = random_bbox_jitter(bbox, image_height, image_width, self.max_bbox_jitter)\n",
    "        image, bbox = random_rotation(image, self.max_rotation, bbox)\n",
    "        image, bbox = random_shear(image, self.max_shear, bbox)\n",
    "        image = tf.image.crop_to_bounding_box(image, bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0])\n",
    "        image = random_shift_and_scale(image, self.max_pixel_shift, self.max_pixel_scale_change)\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        return image, bbox\n",
    "\n",
    "    def _get_files(self, split_file):\n",
    "        \"\"\"Gets image filenames and classes\"\"\"\n",
    "        files, classes, bboxes = [], [], []\n",
    "        with open(split_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                fname, cls, xmin, ymin, xmax, ymax = line.strip('\\n').split()\n",
    "                files.append(os.path.join(self.data_dir, fname))\n",
    "                classes.append(int(cls))\n",
    "                bboxes.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
    "        return files, classes, bboxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_covidnet_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict keys\n",
    "TRAIN_OP_KEY = 'train_op'\n",
    "TF_SUMMARY_KEY = 'tf_summaries'\n",
    "LOSS_KEY = 'loss'\n",
    "\n",
    "# Tensor names\n",
    "IMAGE_INPUT_TENSOR = 'Placeholder:0'\n",
    "LABEL_INPUT_TENSOR = 'Placeholder_1:0'\n",
    "CLASS_PRED_TENSOR = 'ArgMax:0'\n",
    "CLASS_PROB_TENSOR = 'softmax_tensor:0'\n",
    "TRAINING_PH_TENSOR = 'is_training:0'\n",
    "LOSS_TENSOR = 'add:0'\n",
    "\n",
    "# Names for train checkpoints\n",
    "CKPT_NAME = 'model.ckpt'\n",
    "MODEL_NAME = 'COVIDNet-CT'\n",
    "'output'\n",
    "# Output directory for storing runs\n",
    "OUTPUT_DIR = 'output'\n",
    "\n",
    "# Class names ordered by class index\n",
    "CLASS_NAMES = ('Normal', 'Pneumonia', 'COVID-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_grad_filter(gvs):\n",
    "    \"\"\"Filter to apply gradient updates to dense layers only\"\"\"\n",
    "    return [(g, v) for g, v in gvs if 'dense' in v.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_summary(tag_to_value, tag_prefix=''):\n",
    "    \"\"\"Summary object for a dict of python scalars\"\"\"\n",
    "    return tf.Summary(value=[tf.Summary.Value(tag=tag_prefix + tag, simple_value=value)\n",
    "                             for tag, value in tag_to_value.items() if isinstance(value, (int, float))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session():\n",
    "    \"\"\"Helper function for session creation\"\"\"\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "class Metrics:\n",
    "    \"\"\"Lightweight class for tracking metrics\"\"\"\n",
    "    def __init__(self):\n",
    "        num_classes = len(CLASS_NAMES)\n",
    "        self.labels = list(range(num_classes))\n",
    "        self.class_names = CLASS_NAMES\n",
    "        self.confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
    "\n",
    "    def update(self, y_true, y_pred):\n",
    "        self.confusion_matrix = self.confusion_matrix + confusion_matrix(y_true, y_pred, labels=self.labels)\n",
    "\n",
    "    def reset(self):\n",
    "        self.confusion_matrix *= 0\n",
    "\n",
    "    def values(self):\n",
    "        conf_matrix = self.confusion_matrix.astype('float')\n",
    "        metrics = {\n",
    "            'accuracy': np.diag(conf_matrix).sum() / conf_matrix.sum(),\n",
    "            'confusion matrix': self.confusion_matrix.copy()\n",
    "        }\n",
    "        sensitivity = np.diag(conf_matrix) / np.maximum(conf_matrix.sum(axis=1), 1)\n",
    "        pos_pred_val = np.diag(conf_matrix) / np.maximum(conf_matrix.sum(axis=0), 1)\n",
    "        for cls, idx, sens, ppv in zip(self.class_names, self.labels, sensitivity, pos_pred_val):\n",
    "            metrics['{} {}'.format(cls, 'sensitivity')] = sensitivity[idx]\n",
    "            metrics['{} {}'.format(cls, 'PPV')] = pos_pred_val[idx]\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COVIDNetCTRunner:\n",
    "    \"\"\"Primary training/testing/inference class\"\"\"\n",
    "    def __init__(self, meta_file, ckpt=None, data_dir=None, input_height=224, input_width=224, max_bbox_jitter=0.025,\n",
    "                 max_rotation=10, max_shear=0.15, max_pixel_shift=10, max_pixel_scale_change=0.2):\n",
    "        self.meta_file = meta_file\n",
    "        self.ckpt = ckpt\n",
    "        self.input_height = input_height\n",
    "        self.input_width = input_width\n",
    "        if data_dir is None:\n",
    "            self.dataset = None\n",
    "        else:\n",
    "            self.dataset = COVIDxCTDataset(\n",
    "                data_dir,\n",
    "                image_height=input_height,\n",
    "                image_width=input_width,\n",
    "                max_bbox_jitter=max_bbox_jitter,\n",
    "                max_rotation=max_rotation,\n",
    "                max_shear=max_shear,\n",
    "                max_pixel_shift=max_pixel_shift,\n",
    "                max_pixel_scale_change=max_pixel_scale_change\n",
    "            )\n",
    "\n",
    "    def load_graph(self):\n",
    "        \"\"\"Creates new graph and session\"\"\"\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            # Create session and load model\n",
    "            sess = create_session()\n",
    "\n",
    "            # Load meta file\n",
    "            print('Loading meta graph from ' + self.meta_file)\n",
    "            saver = tf.train.import_meta_graph(self.meta_file)\n",
    "        return graph, sess, saver\n",
    "\n",
    "    def load_ckpt(self, sess, saver):\n",
    "        \"\"\"Helper for loading weights\"\"\"\n",
    "        # Load weights\n",
    "        if self.ckpt is not None:\n",
    "            print('Loading weights from ' + self.ckpt)\n",
    "            saver.restore(sess, self.ckpt)\n",
    "\n",
    "    def trainval(self, epochs, output_dir, batch_size=1, learning_rate=0.001, momentum=0.9,\n",
    "                 fc_only=False, train_split_file='train.txt', val_split_file='val.txt',\n",
    "                 log_interval=20, val_interval=1000, save_interval=1000):\n",
    "        \"\"\"Run training with intermittent validation\"\"\"\n",
    "        ckpt_path = os.path.join(output_dir, CKPT_NAME)\n",
    "        graph, sess, saver = self.load_graph()\n",
    "        with graph.as_default():\n",
    "            # Create optimizer\n",
    "            optimizer = tf.train.MomentumOptimizer(\n",
    "                learning_rate=learning_rate,\n",
    "                momentum=momentum\n",
    "            )\n",
    "\n",
    "            # Create train op\n",
    "            global_step = tf.train.get_or_create_global_step()\n",
    "            loss = graph.get_tensor_by_name(LOSS_TENSOR)\n",
    "            grad_vars = optimizer.compute_gradients(loss)\n",
    "            if fc_only:\n",
    "                grad_vars = dense_grad_filter(grad_vars)\n",
    "            minimize_op = optimizer.apply_gradients(grad_vars, global_step)\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            train_op = tf.group(minimize_op, update_ops)\n",
    "\n",
    "            # Load checkpoint\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            self.load_ckpt(sess, saver)\n",
    "\n",
    "            # Create train dataset\n",
    "            dataset, num_images, batch_size = self.dataset.train_dataset(train_split_file, batch_size)\n",
    "            data_next = dataset.make_one_shot_iterator().get_next()\n",
    "            num_iters = ceil(num_images / batch_size) * epochs\n",
    "\n",
    "            # Create feed and fetch dicts\n",
    "            feed_dict = {TRAINING_PH_TENSOR: True}\n",
    "            fetch_dict = {\n",
    "                TRAIN_OP_KEY: train_op,\n",
    "                LOSS_KEY: LOSS_TENSOR\n",
    "            }\n",
    "\n",
    "            # Add summaries\n",
    "            summary_writer = tf.summary.FileWriter(os.path.join(output_dir, 'events'), graph)\n",
    "            fetch_dict[TF_SUMMARY_KEY] = self._get_train_summary_op(graph)\n",
    "\n",
    "            # Create validation function\n",
    "            run_validation = self._get_validation_fn(sess, batch_size, val_split_file)\n",
    "\n",
    "            # Baseline saving and validation\n",
    "            print('Saving baseline checkpoint')\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, ckpt_path, global_step=0)\n",
    "            print('Starting baseline validation')\n",
    "            metrics = run_validation()\n",
    "            self._log_and_print_metrics(metrics, 0, summary_writer)\n",
    "\n",
    "            # Training loop\n",
    "            print('Training with batch_size {} for {} steps'.format(batch_size, num_iters))\n",
    "            for i in range(num_iters):\n",
    "                # Run training step\n",
    "                data = sess.run(data_next)\n",
    "                feed_dict[IMAGE_INPUT_TENSOR] = data['image']\n",
    "                feed_dict[LABEL_INPUT_TENSOR] = data['label']\n",
    "                results = sess.run(fetch_dict, feed_dict)\n",
    "\n",
    "                # Log and save\n",
    "                step = i + 1\n",
    "                if step % log_interval == 0:\n",
    "                    summary_writer.add_summary(results[TF_SUMMARY_KEY], step)\n",
    "                    print('[step: {}, loss: {}]'.format(step, results[LOSS_KEY]))\n",
    "                if step % save_interval == 0:\n",
    "                    print('Saving checkpoint at step {}'.format(step))\n",
    "                    saver.save(sess, ckpt_path, global_step=step)\n",
    "                if val_interval > 0 and step % val_interval == 0:\n",
    "                    print('Starting validation at step {}'.format(step))\n",
    "                    metrics = run_validation()\n",
    "                    self._log_and_print_metrics(metrics, step, summary_writer)\n",
    "\n",
    "            print('Saving checkpoint at last step')\n",
    "            saver.save(sess, ckpt_path, global_step=num_iters)\n",
    "\n",
    "    def test(self, batch_size=1, test_split_file='test.txt', plot_confusion=False):\n",
    "        \"\"\"Run test on a checkpoint\"\"\"\n",
    "        graph, sess, saver = self.load_graph()\n",
    "        with graph.as_default():\n",
    "            # Load checkpoint\n",
    "            self.load_ckpt(sess, saver)\n",
    "\n",
    "            # Run test\n",
    "            print('Starting test')\n",
    "            metrics = self._get_validation_fn(sess, batch_size, test_split_file)()\n",
    "            self._log_and_print_metrics(metrics)\n",
    "\n",
    "            if plot_confusion:\n",
    "                # Plot confusion matrix\n",
    "                fig, ax = plt.subplots()\n",
    "                disp = ConfusionMatrixDisplay(confusion_matrix=metrics['confusion matrix'],\n",
    "                                              display_labels=CLASS_NAMES)\n",
    "                disp.plot(include_values=True, cmap='Blues', ax=ax, xticks_rotation='horizontal', values_format='.5g')\n",
    "                plt.show()\n",
    "\n",
    "    def infer(self, image_file, autocrop=False):\n",
    "        \"\"\"Run inference on the given image\"\"\"\n",
    "        # Load and preprocess image\n",
    "        image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n",
    "        if autocrop:\n",
    "            image, _ = auto_body_crop(image)\n",
    "        image = cv2.resize(image, (self.input_width, self.input_height), cv2.INTER_CUBIC)\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = np.expand_dims(np.stack((image, image, image), axis=-1), axis=0)\n",
    "\n",
    "        # Create feed dict\n",
    "        feed_dict = {IMAGE_INPUT_TENSOR: image, TRAINING_PH_TENSOR: False}\n",
    "\n",
    "        # Run inference\n",
    "        graph, sess, saver = self.load_graph()\n",
    "        with graph.as_default():\n",
    "            # Load checkpoint\n",
    "            self.load_ckpt(sess, saver)\n",
    "\n",
    "            # Run image through model\n",
    "            class_, probs = sess.run([CLASS_PRED_TENSOR, CLASS_PROB_TENSOR], feed_dict=feed_dict)\n",
    "            print('\\nPredicted Class: ' + CLASS_NAMES[class_[0]])\n",
    "            print('Confidences:' + ', '.join(\n",
    "                '{}: {}'.format(name, conf) for name, conf in zip(CLASS_NAMES, probs[0])))\n",
    "            print('**DISCLAIMER**')\n",
    "            print('Do not use this prediction for self-diagnosis. '\n",
    "                  'You should check with your local authorities for '\n",
    "                  'the latest advice on seeking medical assistance.')\n",
    "\n",
    "    def _get_validation_fn(self, sess, batch_size=1, val_split_file='val.txt'):\n",
    "        \"\"\"Creates validation function to call in self.trainval() or self.test()\"\"\"\n",
    "        # Create val dataset\n",
    "        dataset, num_images, batch_size = self.dataset.validation_dataset(val_split_file, batch_size)\n",
    "        dataset = dataset.repeat()  # repeat so there is no need to reconstruct it\n",
    "        data_next = dataset.make_one_shot_iterator().get_next()\n",
    "        num_iters = ceil(num_images / batch_size)\n",
    "\n",
    "        # Create running accuracy metric\n",
    "        metrics = Metrics()\n",
    "\n",
    "        # Create feed and fetch dicts\n",
    "        fetch_dict = {'classes': CLASS_PRED_TENSOR}\n",
    "        feed_dict = {TRAINING_PH_TENSOR: False}\n",
    "\n",
    "        def run_validation():\n",
    "            metrics.reset()\n",
    "            for i in range(num_iters):\n",
    "                data = sess.run(data_next)\n",
    "                feed_dict[IMAGE_INPUT_TENSOR] = data['image']\n",
    "                results = sess.run(fetch_dict, feed_dict)\n",
    "                metrics.update(data['label'], results['classes'])\n",
    "            return metrics.values()\n",
    "\n",
    "        return run_validation\n",
    "\n",
    "    @staticmethod\n",
    "    def _log_and_print_metrics(metrics, step=None, summary_writer=None, tag_prefix='val/'):\n",
    "        \"\"\"Helper for logging and printing\"\"\"\n",
    "        # Pop temporarily and print\n",
    "        cm = metrics.pop('confusion matrix')\n",
    "        print('\\tconfusion matrix:')\n",
    "        print('\\t' + str(cm).replace('\\n', '\\n\\t'))\n",
    "\n",
    "        # Print scalar metrics\n",
    "        for name, val in sorted(metrics.items()):\n",
    "            print('\\t{}: {}'.format(name, val))\n",
    "\n",
    "        # Log scalar metrics\n",
    "        if summary_writer is not None:\n",
    "            summary = simple_summary(metrics, tag_prefix)\n",
    "            summary_writer.add_summary(summary, step)\n",
    "\n",
    "        # Restore confusion matrix\n",
    "        metrics['confusion matrix'] = cm\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_train_summary_op(graph, tag_prefix='train/'):\n",
    "        loss = graph.get_tensor_by_name(LOSS_TENSOR)\n",
    "        loss_summary = tf.summary.scalar(tag_prefix + 'loss', loss)\n",
    "        return loss_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress most console output\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta graph from models/COVIDNet-CT-A/model.meta\n",
      "Loading weights from models/COVIDNet-CT-A/model\n",
      "Saving baseline checkpoint\n",
      "Starting baseline validation\n",
      "\tconfusion matrix:\n",
      "\t[[806   0   0]\n",
      "\t [  0  49   0]\n",
      "\t [  0   1  82]]\n",
      "\tCOVID-19 PPV: 1.0\n",
      "\tCOVID-19 sensitivity: 0.9879518072289156\n",
      "\tNormal PPV: 1.0\n",
      "\tNormal sensitivity: 1.0\n",
      "\tPneumonia PPV: 0.98\n",
      "\tPneumonia sensitivity: 1.0\n",
      "\taccuracy: 0.9989339019189766\n",
      "Training with batch_size 8 for 2880 steps\n",
      "[step: 50, loss: 0.0192182045429945]\n",
      "[step: 100, loss: 0.018801812082529068]\n",
      "[step: 150, loss: 0.019199267029762268]\n",
      "[step: 200, loss: 0.020560581237077713]\n",
      "[step: 250, loss: 0.01901834085583687]\n",
      "[step: 300, loss: 0.019248995929956436]\n",
      "[step: 350, loss: 0.08504898846149445]\n",
      "[step: 400, loss: 0.025765813887119293]\n",
      "[step: 450, loss: 0.020487787202000618]\n",
      "[step: 500, loss: 0.018680505454540253]\n",
      "[step: 550, loss: 0.01867026276886463]\n",
      "[step: 600, loss: 0.018871096894145012]\n",
      "[step: 650, loss: 0.0192891713231802]\n",
      "[step: 700, loss: 0.018718184903264046]\n",
      "[step: 750, loss: 0.01898711733520031]\n",
      "[step: 800, loss: 0.019056886434555054]\n",
      "[step: 850, loss: 0.020415328443050385]\n",
      "[step: 900, loss: 0.018874725326895714]\n",
      "[step: 950, loss: 0.04141579940915108]\n",
      "[step: 1000, loss: 0.018751071766018867]\n",
      "[step: 1050, loss: 0.019141964614391327]\n",
      "[step: 1100, loss: 0.019264737144112587]\n",
      "[step: 1150, loss: 0.01929694600403309]\n",
      "[step: 1200, loss: 0.018834851682186127]\n",
      "[step: 1250, loss: 0.01925419270992279]\n",
      "[step: 1300, loss: 0.01869775541126728]\n",
      "[step: 1350, loss: 0.03735556825995445]\n",
      "[step: 1400, loss: 0.036839842796325684]\n",
      "[step: 1450, loss: 0.021368887275457382]\n",
      "[step: 1500, loss: 0.019712334498763084]\n",
      "[step: 1550, loss: 0.02107291668653488]\n",
      "[step: 1600, loss: 0.021201318129897118]\n",
      "[step: 1650, loss: 0.06731949746608734]\n",
      "[step: 1700, loss: 0.01873074471950531]\n",
      "[step: 1750, loss: 0.018712980672717094]\n",
      "[step: 1800, loss: 0.025428306311368942]\n",
      "[step: 1850, loss: 0.018989551812410355]\n",
      "[step: 1900, loss: 0.03478431701660156]\n",
      "[step: 1950, loss: 0.019616814330220222]\n",
      "[step: 2000, loss: 0.01871275156736374]\n",
      "Saving checkpoint at step 2000\n",
      "Starting validation at step 2000\n",
      "\tconfusion matrix:\n",
      "\t[[806   0   0]\n",
      "\t [  2  47   0]\n",
      "\t [  2   1  80]]\n",
      "\tCOVID-19 PPV: 1.0\n",
      "\tCOVID-19 sensitivity: 0.963855421686747\n",
      "\tNormal PPV: 0.9950617283950617\n",
      "\tNormal sensitivity: 1.0\n",
      "\tPneumonia PPV: 0.9791666666666666\n",
      "\tPneumonia sensitivity: 0.9591836734693877\n",
      "\taccuracy: 0.9946695095948828\n",
      "[step: 2050, loss: 0.019885875284671783]\n",
      "[step: 2100, loss: 0.018667444586753845]\n",
      "[step: 2150, loss: 0.026125021278858185]\n",
      "[step: 2200, loss: 0.03192813694477081]\n",
      "[step: 2250, loss: 0.0190891120582819]\n",
      "[step: 2300, loss: 0.040915973484516144]\n",
      "[step: 2350, loss: 0.023052485659718513]\n",
      "[step: 2400, loss: 0.022381171584129333]\n",
      "[step: 2450, loss: 0.01966536045074463]\n",
      "[step: 2500, loss: 0.02211478352546692]\n",
      "[step: 2550, loss: 0.019838541746139526]\n",
      "[step: 2600, loss: 0.03469415009021759]\n",
      "[step: 2650, loss: 0.021289126947522163]\n",
      "[step: 2700, loss: 0.02053600363433361]\n",
      "[step: 2750, loss: 0.018678374588489532]\n",
      "[step: 2800, loss: 0.024852223694324493]\n",
      "[step: 2850, loss: 0.17683181166648865]\n",
      "Saving checkpoint at last step\n"
     ]
    }
   ],
   "source": [
    "# mode = 'train' \n",
    "\n",
    "# Create full paths\n",
    "model_dir = 'models/COVIDNet-CT-A'\n",
    "meta_file = 'models/COVIDNet-CT-A/model.meta'\n",
    "ckpt = 'models/COVIDNet-CT-A/model'\n",
    "input_height = 512\n",
    "input_width = 512\n",
    "\n",
    "# Create runner\n",
    "if mode == 'train':\n",
    "    augmentation_kwargs = dict(\n",
    "        max_bbox_jitter = 0.075,\n",
    "        max_rotation = 15,\n",
    "        max_shear = 0.2,\n",
    "        max_pixel_shift = 15,\n",
    "        max_pixel_scale_change = 0.15\n",
    "    )\n",
    "else:\n",
    "    augmentation_kwargs = {}\n",
    "        \n",
    "runner = COVIDNetCTRunner(\n",
    "    meta_file,\n",
    "    ckpt = ckpt,\n",
    "    data_dir = 'data/COVIDx-CT',\n",
    "    input_height = input_height,\n",
    "    input_width = input_width,\n",
    "    max_bbox_jitter = 0.075,\n",
    "    max_rotation = 15,\n",
    "    max_shear = 0.2,\n",
    "    max_pixel_shift = 15,\n",
    "    max_pixel_scale_change = 0.15\n",
    ")\n",
    "\n",
    "if mode == 'train':\n",
    "    output_suffix = datetime.now().strftime('_%Y-%m-%d_%H.%M.%S')\n",
    "    # Create output_dir and save run settings\n",
    "    output_dir = os.path.join(OUTPUT_DIR, MODEL_NAME + output_suffix)\n",
    "    os.makedirs(output_dir, exist_ok=False)\n",
    "    \n",
    "    data_dir = 'data/COVIDx-CT'\n",
    "    train_split_file = 'train_COVIDx-CT.txt'\n",
    "    val_split_file = 'val_COVIDx-CT.txt'\n",
    "    epochs = 20\n",
    "    batch_size = 8\n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.9\n",
    "    fc_only = 'store_true'\n",
    "    log_interval = 50\n",
    "    val_interval = 2000\n",
    "    save_interval = 2000\n",
    "    \n",
    "    args = {\"model_dir\": \"models/COVIDNet-CT-A\", \"meta_name\": \"model.meta\", \"ckpt_name\": \"model\", \"input_height\": 512, \\\n",
    "     \"input_width\": 512, \"output_suffix\": output_suffix, \"data_dir\": \"data/COVIDx-CT\", \\\n",
    "     \"train_split_file\": \"train_COVIDx-CT.txt\", \"val_split_file\": \"val_COVIDx-CT.txt\", \"epochs\": 20, \"batch_size\": 8,\\\n",
    "     \"learning_rate\": 0.001, \"momentum\": 0.9, \"fc_only\": fc_only, \"log_interval\": 50, \"val_interval\": 2000,\\\n",
    "     \"save_interval\": 2000, \"max_bbox_jitter\": 0.075, \"max_rotation\": 15, \"max_shear\": 0.2, \\\n",
    "     \"max_pixel_shift\": 15, \"max_pixel_scale_change\": 0.15}\n",
    "\n",
    "    # Run trainval\n",
    "    runner.trainval(\n",
    "        epochs,\n",
    "        output_dir,\n",
    "        batch_size = batch_size,\n",
    "        learning_rate = learning_rate,\n",
    "        momentum = momentum,\n",
    "        fc_only = fc_only,\n",
    "        train_split_file = train_split_file,\n",
    "        val_split_file = val_split_file,\n",
    "        log_interval = log_interval,\n",
    "        val_interval = val_interval,\n",
    "        save_interval = save_interval\n",
    "    )\n",
    "elif mode == 'test':\n",
    "    data_dir = 'data/COVIDx-CT'\n",
    "    batch_size = 8\n",
    "    test_split_file = 'test_COVIDx-CT.txt'\n",
    "    plot_confusion = 'store_true'\n",
    "    \n",
    "    # Run validation\n",
    "    runner.test(\n",
    "        batch_size = batch_size,\n",
    "        test_split_file = test_split_file,\n",
    "        plot_confusion = plot_confusion\n",
    "    )\n",
    "elif mode == 'infer':\n",
    "    image_file = 'assets/ex-covid-ct.png'\n",
    "    auto_crop = 'store_true'\n",
    "    # Run inference\n",
    "    runner.infer(image_file, auto_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta graph from models/COVIDNet-CT-A/model.meta\n",
      "Loading weights from models/COVIDNet-CT-A/model\n",
      "Starting test\n",
      "\tconfusion matrix:\n",
      "\t[[148   0   0]\n",
      "\t [  0  42   0]\n",
      "\t [  0  20 110]]\n",
      "\tCOVID-19 PPV: 1.0\n",
      "\tCOVID-19 sensitivity: 0.8461538461538461\n",
      "\tNormal PPV: 1.0\n",
      "\tNormal sensitivity: 1.0\n",
      "\tPneumonia PPV: 0.6774193548387096\n",
      "\tPneumonia sensitivity: 1.0\n",
      "\taccuracy: 0.9375\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mode = 'test' \n",
    "\n",
    "# Create full paths\n",
    "model_dir = 'models/COVIDNet-CT-A'\n",
    "meta_file = 'models/COVIDNet-CT-A/model.meta'\n",
    "ckpt = 'models/COVIDNet-CT-A/model'\n",
    "input_height = 512\n",
    "input_width = 512\n",
    "\n",
    "# Create runner\n",
    "if mode == 'train':\n",
    "    augmentation_kwargs = dict(\n",
    "        max_bbox_jitter = 0.075,\n",
    "        max_rotation = 15,\n",
    "        max_shear = 0.2,\n",
    "        max_pixel_shift = 15,\n",
    "        max_pixel_scale_change = 0.15\n",
    "    )\n",
    "else:\n",
    "    augmentation_kwargs = {}\n",
    "        \n",
    "runner = COVIDNetCTRunner(\n",
    "    meta_file,\n",
    "    ckpt = ckpt,\n",
    "    data_dir = 'data/COVIDx-CT',\n",
    "    input_height = input_height,\n",
    "    input_width = input_width,\n",
    "    max_bbox_jitter = 0.075,\n",
    "    max_rotation = 15,\n",
    "    max_shear = 0.2,\n",
    "    max_pixel_shift = 15,\n",
    "    max_pixel_scale_change = 0.15\n",
    ")\n",
    "\n",
    "if mode == 'train':\n",
    "    output_suffix = datetime.now().strftime('_%Y-%m-%d_%H.%M.%S')\n",
    "    # Create output_dir and save run settings\n",
    "    output_dir = os.path.join(OUTPUT_DIR, MODEL_NAME + output_suffix)\n",
    "    os.makedirs(output_dir, exist_ok=False)\n",
    "    \n",
    "    data_dir = 'data/COVIDx-CT'\n",
    "    train_split_file = 'train_COVIDx-CT.txt'\n",
    "    val_split_file = 'val_COVIDx-CT.txt'\n",
    "    epochs = 20\n",
    "    batch_size = 8\n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.9\n",
    "    fc_only = 'store_true'\n",
    "    log_interval = 50\n",
    "    val_interval = 2000\n",
    "    save_interval = 2000\n",
    "    \n",
    "    args = {\"model_dir\": \"models/COVIDNet-CT-A\", \"meta_name\": \"model.meta\", \"ckpt_name\": \"model\", \"input_height\": 512, \\\n",
    "     \"input_width\": 512, \"output_suffix\": output_suffix, \"data_dir\": \"data/COVIDx-CT\", \\\n",
    "     \"train_split_file\": \"train_COVIDx-CT.txt\", \"val_split_file\": \"val_COVIDx-CT.txt\", \"epochs\": 20, \"batch_size\": 8,\\\n",
    "     \"learning_rate\": 0.001, \"momentum\": 0.9, \"fc_only\": fc_only, \"log_interval\": 50, \"val_interval\": 2000,\\\n",
    "     \"save_interval\": 2000, \"max_bbox_jitter\": 0.075, \"max_rotation\": 15, \"max_shear\": 0.2, \\\n",
    "     \"max_pixel_shift\": 15, \"max_pixel_scale_change\": 0.15}\n",
    "\n",
    "    # Run trainval\n",
    "    runner.trainval(\n",
    "        epochs,\n",
    "        output_dir,\n",
    "        batch_size = batch_size,\n",
    "        learning_rate = learning_rate,\n",
    "        momentum = momentum,\n",
    "        fc_only = fc_only,\n",
    "        train_split_file = train_split_file,\n",
    "        val_split_file = val_split_file,\n",
    "        log_interval = log_interval,\n",
    "        val_interval = val_interval,\n",
    "        save_interval = save_interval\n",
    "    )\n",
    "elif mode == 'test':\n",
    "    data_dir = 'data/COVIDx-CT'\n",
    "    batch_size = 8\n",
    "    test_split_file = 'test_COVIDx-CT.txt'\n",
    "    plot_confusion = 'store_true'\n",
    "    \n",
    "    # Run validation\n",
    "    runner.test(\n",
    "        batch_size = batch_size,\n",
    "        test_split_file = test_split_file,\n",
    "        plot_confusion = plot_confusion\n",
    "    )\n",
    "elif mode == 'infer':\n",
    "    image_file = 'assets/ex-covid-ct.png'\n",
    "    auto_crop = 'store_true'\n",
    "    # Run inference\n",
    "    runner.infer(image_file, auto_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'infer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta graph from models/COVIDNet-CT-A/model.meta\n",
      "Loading weights from models/COVIDNet-CT-A/model\n",
      "\n",
      "Predicted Class: COVID-19\n",
      "Confidences:Normal: 4.3196553178859176e-07, Pneumonia: 0.001139980275183916, COVID-19: 0.9988596439361572\n",
      "**DISCLAIMER**\n",
      "Do not use this prediction for self-diagnosis. You should check with your local authorities for the latest advice on seeking medical assistance.\n"
     ]
    }
   ],
   "source": [
    "# mode = 'infer' \n",
    "\n",
    "# Create full paths\n",
    "model_dir = 'models/COVIDNet-CT-A'\n",
    "meta_file = 'models/COVIDNet-CT-A/model.meta'\n",
    "ckpt = 'models/COVIDNet-CT-A/model'\n",
    "input_height = 512\n",
    "input_width = 512\n",
    "\n",
    "# Create runner\n",
    "if mode == 'train':\n",
    "    augmentation_kwargs = dict(\n",
    "        max_bbox_jitter = 0.075,\n",
    "        max_rotation = 15,\n",
    "        max_shear = 0.2,\n",
    "        max_pixel_shift = 15,\n",
    "        max_pixel_scale_change = 0.15\n",
    "    )\n",
    "else:\n",
    "    augmentation_kwargs = {}\n",
    "        \n",
    "runner = COVIDNetCTRunner(\n",
    "    meta_file,\n",
    "    ckpt = ckpt,\n",
    "    data_dir = 'data/COVIDx-CT',\n",
    "    input_height = input_height,\n",
    "    input_width = input_width,\n",
    "    max_bbox_jitter = 0.075,\n",
    "    max_rotation = 15,\n",
    "    max_shear = 0.2,\n",
    "    max_pixel_shift = 15,\n",
    "    max_pixel_scale_change = 0.15\n",
    ")\n",
    "\n",
    "if mode == 'train':\n",
    "    output_suffix = datetime.now().strftime('_%Y-%m-%d_%H.%M.%S')\n",
    "    # Create output_dir and save run settings\n",
    "    output_dir = os.path.join(OUTPUT_DIR, MODEL_NAME + output_suffix)\n",
    "    os.makedirs(output_dir, exist_ok=False)\n",
    "    \n",
    "    data_dir = 'data/COVIDx-CT'\n",
    "    train_split_file = 'train_COVIDx-CT.txt'\n",
    "    val_split_file = 'val_COVIDx-CT.txt'\n",
    "    epochs = 20\n",
    "    batch_size = 8\n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.9\n",
    "    fc_only = 'store_true'\n",
    "    log_interval = 50\n",
    "    val_interval = 2000\n",
    "    save_interval = 2000\n",
    "    \n",
    "    args = {\"model_dir\": \"models/COVIDNet-CT-A\", \"meta_name\": \"model.meta\", \"ckpt_name\": \"model\", \"input_height\": 512, \\\n",
    "     \"input_width\": 512, \"output_suffix\": output_suffix, \"data_dir\": \"data/COVIDx-CT\", \\\n",
    "     \"train_split_file\": \"train_COVIDx-CT.txt\", \"val_split_file\": \"val_COVIDx-CT.txt\", \"epochs\": 20, \"batch_size\": 8,\\\n",
    "     \"learning_rate\": 0.001, \"momentum\": 0.9, \"fc_only\": fc_only, \"log_interval\": 50, \"val_interval\": 2000,\\\n",
    "     \"save_interval\": 2000, \"max_bbox_jitter\": 0.075, \"max_rotation\": 15, \"max_shear\": 0.2, \\\n",
    "     \"max_pixel_shift\": 15, \"max_pixel_scale_change\": 0.15}\n",
    "\n",
    "    # Run trainval\n",
    "    runner.trainval(\n",
    "        epochs,\n",
    "        output_dir,\n",
    "        batch_size = batch_size,\n",
    "        learning_rate = learning_rate,\n",
    "        momentum = momentum,\n",
    "        fc_only = fc_only,\n",
    "        train_split_file = train_split_file,\n",
    "        val_split_file = val_split_file,\n",
    "        log_interval = log_interval,\n",
    "        val_interval = val_interval,\n",
    "        save_interval = save_interval\n",
    "    )\n",
    "elif mode == 'test':\n",
    "    data_dir = 'data/COVIDx-CT'\n",
    "    batch_size = 8\n",
    "    test_split_file = 'test_COVIDx-CT.txt'\n",
    "    plot_confusion = 'store_true'\n",
    "    \n",
    "    # Run validation\n",
    "    runner.test(\n",
    "        batch_size = batch_size,\n",
    "        test_split_file = test_split_file,\n",
    "        plot_confusion = plot_confusion\n",
    "    )\n",
    "elif mode == 'infer':\n",
    "    image_file = 'assets/ex-covid-ct.png'\n",
    "    auto_crop = 'store_true'\n",
    "    # Run inference\n",
    "    runner.infer(image_file, auto_crop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
