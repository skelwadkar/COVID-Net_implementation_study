{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import sklearn\n",
    "import glob\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "#import matplotlib as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hu_to_uint8(hu_images, window_width, window_center):\n",
    "    \"\"\"Converts HU images to uint8 images\"\"\"\n",
    "    images = (hu_images.astype(np.float) - window_center + window_width/2)/window_width\n",
    "    uint8_images = np.uint8(255.0*np.clip(images, 0.0, 1.0))\n",
    "    return uint8_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_uint8(data, window_width=1500, window_center=-600):\n",
    "    \"\"\"Converts non-uint8 data to uint8 and applies window level to HU data\"\"\"\n",
    "    if data.dtype != np.uint8:\n",
    "        if data.ptp() > 255:\n",
    "            # Assume HU\n",
    "            data = hu_to_uint8(data, window_width, window_center)\n",
    "        data = data.astype(np.uint8)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contours(binary_image):\n",
    "    \"\"\"Helper function for finding contours\"\"\"\n",
    "    return cv2.findContours(binary_image, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_contour(binary_image):\n",
    "    \"\"\"Helper function to get body contour\"\"\"\n",
    "    contours = find_contours(binary_image)\n",
    "    areas = [cv2.contourArea(cnt) for cnt in contours]\n",
    "    body_idx = np.argmax(areas)\n",
    "    return contours[body_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_body_crop(image, scale=1.0):\n",
    "    \"\"\"Roughly crop an image to the body region\"\"\"\n",
    "    # Create initial binary image\n",
    "    filt_image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    thresh = cv2.threshold(filt_image[filt_image > 0], 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[0]\n",
    "    bin_image = np.uint8(filt_image > thresh)\n",
    "    erode_kernel = np.ones((7, 7), dtype=np.uint8)\n",
    "    bin_image = cv2.erode(bin_image, erode_kernel)\n",
    "\n",
    "    # Find body contour\n",
    "    body_cont = body_contour(bin_image).squeeze()\n",
    "\n",
    "    # Get bbox\n",
    "    xmin = body_cont[:, 0].min()\n",
    "    xmax = body_cont[:, 0].max() + 1\n",
    "    ymin = body_cont[:, 1].min()\n",
    "    ymax = body_cont[:, 1].max() + 1\n",
    "\n",
    "    # Scale to final bbox\n",
    "    if scale > 0 and scale != 1.0:\n",
    "        center = ((xmax + xmin)/2, (ymin + ymax)/2)\n",
    "        width = scale*(xmax - xmin + 1)\n",
    "        height = scale*(ymax - ymin + 1)\n",
    "        xmin = int(center[0] - width/2)\n",
    "        xmax = int(center[0] + width/2)\n",
    "        ymin = int(center[1] - height/2)\n",
    "        ymax = int(center[1] + height/2)\n",
    "\n",
    "    return image[ymin:ymax, xmin:xmax], (xmin, ymin, xmax, ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_ext_file_iter(directory, extensions):\n",
    "    \"\"\"Creates a multi-extension file iterator\"\"\"\n",
    "    patterns = ['*.' + ext.lower() for ext in extensions]\n",
    "    return itertools.chain.from_iterable(\n",
    "        glob.iglob(os.path.join(directory, pat)) for pat in patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = ('png', 'jpg', 'jpeg', 'tif')\n",
    "CLASS_MAP = {'Normal': 0, 'CP': 1, 'NCP': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lesion_files(lesion_file, exclude_list, root_dir):\n",
    "    \"\"\"Reads the lesion file to identify relevant\n",
    "    slices and returns their paths\"\"\"\n",
    "    files = []\n",
    "    with open(lesion_file, 'r') as f:\n",
    "        f.readline()\n",
    "        for line in f.readlines():\n",
    "            cls, pid = line.split('/')[:2]\n",
    "            if pid not in exclude_list:\n",
    "                files.append(os.path.join(root_dir, line.strip('\\n'))) ###*********###\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(lesion_file, unzip_file, exclude_file, root_dir):\n",
    "    \"\"\"Gets image file paths according to given lists\"\"\"\n",
    "    excluded_pids = get_excluded_pids(exclude_file)\n",
    "    files = get_lesion_files(lesion_file, excluded_pids['CP'] + excluded_pids['NCP'], root_dir)\n",
    "    with open(unzip_file, 'r') as f:\n",
    "        reader = list(csv.DictReader(f, delimiter=',', quotechar='|'))\n",
    "        for row in reader:\n",
    "            if row['label'] == 'Normal':\n",
    "                pid = row['patient_id']\n",
    "                sid = row['scan_id']\n",
    "                if pid not in excluded_pids['Normal']:\n",
    "                    files += get_source_paths(root_dir, 'Normal', pid, sid)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_paths(root_dir, cls, pid, sid):\n",
    "    \"\"\"Helper function to construct source paths\"\"\"\n",
    "    exam_dir = os.path.join(root_dir, cls, pid, sid)\n",
    "    return list(multi_ext_file_iter(exam_dir, IMG_EXTENSIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output_path(output_dir, source_path):\n",
    "    \"\"\"Helper function to construct output paths\"\"\"\n",
    "    source_path = source_path.replace('\\\\', '/')\n",
    "    parts = source_path.split('/')[-4:]\n",
    "    output_path = os.path.join(output_dir, '_'.join(parts))\n",
    "    output_path = os.path.splitext(output_path)[0] + '.png'\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imread_gray(path):\n",
    "    \"\"\"Reads images in grayscale\"\"\"\n",
    "    image = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    if image.ndim > 2:\n",
    "        image = image[:, :, 0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_excluded_pids(exclude_file):\n",
    "    \"\"\"Reads the exclusion list and returns a\n",
    "    dict of lists of excluded patients\"\"\"\n",
    "    exclude_pids = {\n",
    "        'NCP': [],\n",
    "        'CP': [],\n",
    "        'Normal': []\n",
    "    }\n",
    "    with open(exclude_file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            cls, pid = line.strip('\\n').split()\n",
    "            exclude_pids[cls].append(pid)\n",
    "    return exclude_pids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"parser = argparse.ArgumentParser()\\nparser.add_argument('root_dir', type=str,\\n                    help='Root directory with NCP, CP, and Normal directories, as well as metadata files')\\nparser.add_argument('-o', '--output_dir', type=str, default='data/COVIDx-CT', help='Directory to construct dataset in')\\nparser.add_argument('-l', '--lesion_file', type=str, default='lesions_slices.csv',\\n                    help='CSV file indicating slices with lesions')\\nparser.add_argument('-u', '--unzip_file', type=str, default='unzip_filenames.csv',\\n                    help='CSV file indicating unzipped filenames')\\nparser.add_argument('-e', '--exclude_file', type=str, default='exclude_list.txt',\\n                    help='Text file indicating patient IDs to skip')\\nargs = parser.parse_args()\\n\\nos.makedirs(args.output_dir, exist_ok=True)\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''parser = argparse.ArgumentParser()\n",
    "parser.add_argument('root_dir', type=str,\n",
    "                    help='Root directory with NCP, CP, and Normal directories, as well as metadata files')\n",
    "parser.add_argument('-o', '--output_dir', type=str, default='data/COVIDx-CT', help='Directory to construct dataset in')\n",
    "parser.add_argument('-l', '--lesion_file', type=str, default='lesions_slices.csv',\n",
    "                    help='CSV file indicating slices with lesions')\n",
    "parser.add_argument('-u', '--unzip_file', type=str, default='unzip_filenames.csv',\n",
    "                    help='CSV file indicating unzipped filenames')\n",
    "parser.add_argument('-e', '--exclude_file', type=str, default='exclude_list.txt',\n",
    "                    help='Text file indicating patient IDs to skip')\n",
    "args = parser.parse_args()\n",
    "\n",
    "os.makedirs(args.output_dir, exist_ok=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'C:/Users/sonal/OneDrive/Desktop/Covid19_project/CovidNet/COVIDNet-CT-master'\n",
    "output_dir = 'C:/Users/sonal/OneDrive/Desktop/Covid19_project/CovidNet/COVIDNet-CT-master/output'\n",
    "lesion_file = 'C:/Users/sonal/OneDrive/Desktop/Covid19_project/CovidNet/COVIDNet-CT-master/lesions_slices.csv'\n",
    "unzip_file = 'C:/Users/sonal/OneDrive/Desktop/Covid19_project/CovidNet/COVIDNet-CT-master/unzip_filenames.csv'\n",
    "exclude_file = 'C:/Users/sonal/OneDrive/Desktop/Covid19_project/CovidNet/COVIDNet-CT-master/exclude_list.txt'\n",
    "image_files = get_files(lesion_file, unzip_file, exclude_file, root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to new files as PNGs\n",
    "for imf in image_files:\n",
    "    image = imread_gray(imf)\n",
    "    output_path = make_output_path(output_dir, imf)\n",
    "    cv2.imwrite(output_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python prepare_data.py C:/Users/sonal/OneDrive/Desktop/Covid19_project/CovidNet/COVIDNet-CT-master -o C:/Users/sonal/OneDrive/Desktop/Covid19_project/CovidNet/COVIDNet-CT-master/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotation(image, max_degrees, bbox=None, prob=0.5):\n",
    "    \"\"\"Applies random rotation to image and bbox\"\"\"\n",
    "    def _rotation(image, bbox):\n",
    "        # Get random angle\n",
    "        degrees = tf.random.uniform([], minval=-max_degrees, maxval=max_degrees, dtype=tf.float32)\n",
    "        radians = degrees * math.pi / 180.\n",
    "        if bbox is not None:\n",
    "            # Get offset from image center\n",
    "            image_shape = tf.cast(tf.shape(image), tf.float32)\n",
    "            image_height, image_width = image_shape[0], image_shape[1]\n",
    "            bbox = tf.cast(bbox, tf.float32)\n",
    "            center_x = image_width / 2.\n",
    "            center_y = image_height / 2.\n",
    "            bbox_center_x = (bbox[0] + bbox[2]) / 2.\n",
    "            bbox_center_y = (bbox[1] + bbox[3]) / 2.\n",
    "            trans_x = center_x - bbox_center_x\n",
    "            trans_y = center_y - bbox_center_y\n",
    "\n",
    "            # Apply rotation\n",
    "            image = _translate_image(image, trans_x, trans_y)\n",
    "            bbox = _translate_bbox(bbox, image_height, image_width, trans_x, trans_y)\n",
    "            image = tf.contrib.image.rotate(image, radians, interpolation='BILINEAR')\n",
    "            bbox = _rotate_bbox(bbox, image_height, image_width, radians)\n",
    "            image = _translate_image(image, -trans_x, -trans_y)\n",
    "            bbox = _translate_bbox(bbox, image_height, image_width, -trans_x, -trans_y)\n",
    "            bbox = tf.cast(bbox, tf.int32)\n",
    "\n",
    "            return image, bbox\n",
    "        return tf.contrib.image.rotate(image, radians, interpolation='BILINEAR')\n",
    "\n",
    "    retval = image if bbox is None else (image, bbox)\n",
    "    return tf.cond(_should_apply(prob), lambda: _rotation(image, bbox), lambda: retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_bbox_jitter(bbox, image_height, image_width, max_fraction, prob=0.5):\n",
    "    \"\"\"Jitters bbox coordinates by +/- jitter_fraction of the width/height\"\"\"\n",
    "    def _bbox_jitter(bbox):\n",
    "        bbox = tf.cast(bbox, tf.float32)\n",
    "        width_jitter = max_fraction*(bbox[2] - bbox[0])\n",
    "        height_jitter = max_fraction*(bbox[3] - bbox[1])\n",
    "        xmin = bbox[0] + tf.random.uniform([], minval=-width_jitter, maxval=width_jitter, dtype=tf.float32)\n",
    "        ymin = bbox[1] + tf.random.uniform([], minval=-height_jitter, maxval=height_jitter, dtype=tf.float32)\n",
    "        xmax = bbox[2] + tf.random.uniform([], minval=-width_jitter, maxval=width_jitter, dtype=tf.float32)\n",
    "        ymax = bbox[3] + tf.random.uniform([], minval=-height_jitter, maxval=height_jitter, dtype=tf.float32)\n",
    "        xmin, ymin, xmax, ymax = _clip_bbox(xmin, ymin, xmax, ymax, image_height, image_width)\n",
    "        bbox = tf.cast(tf.stack([xmin, ymin, xmax, ymax]), tf.int32)\n",
    "        return bbox\n",
    "\n",
    "    return tf.cond(_should_apply(prob), lambda: _bbox_jitter(bbox), lambda: bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shift_and_scale(image, max_shift, max_scale_change, prob=0.5):\n",
    "    \"\"\"Applies random shift and scale to pixel values\"\"\"\n",
    "    def _shift_and_scale(image):\n",
    "        shift = tf.cast(tf.random.uniform([], minval=-max_shift, maxval=max_shift, dtype=tf.int32), tf.float32)\n",
    "        scale = tf.random.uniform([], minval=(1. - max_scale_change),\n",
    "                                  maxval=(1. + max_scale_change), dtype=tf.float32)\n",
    "        image = scale*(tf.cast(image, tf.float32) + shift)\n",
    "        image = tf.cast(tf.clip_by_value(image, 0., 255.), tf.uint8)\n",
    "        return image\n",
    "\n",
    "    return tf.cond(_should_apply(prob), lambda: _shift_and_scale(image), lambda: image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shear(image, max_lambda, bbox=None, prob=0.5):\n",
    "    \"\"\"Applies shear in either the x or y direction\"\"\"\n",
    "    shear_lambda = tf.random.uniform([], minval=-max_lambda, maxval=max_lambda, dtype=tf.float32)\n",
    "    image_shape = tf.cast(tf.shape(image), tf.float32)\n",
    "    image_height, image_width = image_shape[0], image_shape[1]\n",
    "\n",
    "    def _shear_x(image, bbox):\n",
    "        image = _shear_x_image(image, shear_lambda)\n",
    "        if bbox is not None:\n",
    "            bbox = _shear_bbox(bbox, image_height, image_width, shear_lambda, horizontal=True)\n",
    "            bbox = tf.cast(bbox, tf.int32)\n",
    "            return image, bbox\n",
    "        return image\n",
    "\n",
    "    def _shear_y(image, bbox):\n",
    "        image = _shear_y_image(image, shear_lambda)\n",
    "        if bbox is not None:\n",
    "            bbox = _shear_bbox(bbox, image_height, image_width, shear_lambda, horizontal=False)\n",
    "            bbox = tf.cast(bbox, tf.int32)\n",
    "            return image, bbox\n",
    "        return image\n",
    "\n",
    "    def _shear(image, bbox):\n",
    "        return tf.cond(_should_apply(0.5), lambda: _shear_x(image, bbox), lambda: _shear_y(image, bbox))\n",
    "\n",
    "    retval = image if bbox is None else (image, bbox)\n",
    "    return tf.cond(_should_apply(prob), lambda: _shear(image, bbox), lambda: retval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _translate_image(image, delta_x, delta_y):\n",
    "    \"\"\"Translate an image\"\"\"\n",
    "    return tf.contrib.image.translate(image, [delta_x, delta_y], interpolation='BILINEAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _translate_bbox(bbox, image_height, image_width, delta_x, delta_y):\n",
    "    \"\"\"Translate an bbox, ensuring coordinates lie in the image\"\"\"\n",
    "    bbox = bbox + tf.stack([delta_x, delta_y, delta_x, delta_y])\n",
    "    xmin, ymin, xmax, ymax = _clip_bbox(bbox[0], bbox[1], bbox[2], bbox[3], image_height, image_width)\n",
    "    bbox = tf.stack([xmin, ymin, xmax, ymax])\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rotate_bbox(bbox, image_height, image_width, radians):\n",
    "    \"\"\"Rotates the bbox by the given angle\"\"\"\n",
    "    # Shift bbox to origin\n",
    "    xmin, ymin, xmax, ymax = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "    center_x = (xmin + xmax) / 2.\n",
    "    center_y = (ymin + ymax) / 2.\n",
    "    xmin = xmin - center_x\n",
    "    xmax = xmax - center_x\n",
    "    ymin = ymin - center_y\n",
    "    ymax = ymax - center_y\n",
    "\n",
    "    # Rotate bbox coordinates\n",
    "    radians = -radians  # negate direction since y-axis is flipped\n",
    "    coords = tf.stack([[xmin, ymin], [xmax, ymin], [xmin, ymax], [xmax, ymax]])\n",
    "    coords = tf.transpose(tf.cast(coords, tf.float32))\n",
    "    rotation_matrix = tf.stack(\n",
    "        [[tf.cos(radians), -tf.sin(radians)],\n",
    "         [tf.sin(radians), tf.cos(radians)]])\n",
    "    new_coords = tf.matmul(rotation_matrix, coords)\n",
    "\n",
    "    # Find new bbox coordinates and clip to image size\n",
    "    xmin = tf.reduce_min(new_coords[0, :]) + center_x\n",
    "    ymin = tf.reduce_min(new_coords[1, :]) + center_y\n",
    "    xmax = tf.reduce_max(new_coords[0, :]) + center_x\n",
    "    ymax = tf.reduce_max(new_coords[1, :]) + center_y\n",
    "    xmin, ymin, xmax, ymax = _clip_bbox(xmin, ymin, xmax, ymax, image_height, image_width)\n",
    "    bbox = tf.stack([xmin, ymin, xmax, ymax])\n",
    "\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shear_x_image(image, shear_lambda):\n",
    "    \"\"\"Shear image in x-direction\"\"\"\n",
    "    tform = tf.stack([1., shear_lambda, 0., 0., 1., 0., 0., 0.])\n",
    "    image = tf.contrib.image.transform(\n",
    "        image, tform, interpolation='BILINEAR')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shear_y_image(image, shear_lambda):\n",
    "    \"\"\"Shear image in y-direction\"\"\"\n",
    "    tform = tf.stack([1., 0., 0., shear_lambda, 1., 0., 0., 0.])\n",
    "    image = tf.contrib.image.transform(\n",
    "        image, tform, interpolation='BILINEAR')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shear_bbox(bbox, image_height, image_width, shear_lambda, horizontal=True):\n",
    "    \"\"\"Shear bbox in x- or y-direction\"\"\"\n",
    "    # Shear bbox coordinates\n",
    "    xmin, ymin, xmax, ymax = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "    coords = tf.stack([[xmin, ymin], [xmax, ymin], [xmin, ymax], [xmax, ymax]])\n",
    "    coords = tf.transpose(tf.cast(coords, tf.float32))\n",
    "    if horizontal:\n",
    "        shear_matrix = tf.stack(\n",
    "            [[1., -shear_lambda],\n",
    "             [0., 1.]])\n",
    "    else:\n",
    "        shear_matrix = tf.stack(\n",
    "            [[1., 0.],\n",
    "             [-shear_lambda, 1.]])\n",
    "    new_coords = tf.matmul(shear_matrix, coords)\n",
    "\n",
    "    # Find new bbox coordinates and clip to image size\n",
    "    xmin = tf.reduce_min(new_coords[0, :])\n",
    "    ymin = tf.reduce_min(new_coords[1, :])\n",
    "    xmax = tf.reduce_max(new_coords[0, :])\n",
    "    ymax = tf.reduce_max(new_coords[1, :])\n",
    "    xmin, ymin, xmax, ymax = _clip_bbox(xmin, ymin, xmax, ymax, image_height, image_width)\n",
    "    bbox = tf.stack([xmin, ymin, xmax, ymax])\n",
    "\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clip_bbox(xmin, ymin, xmax, ymax, image_height, image_width):\n",
    "    \"\"\"Clip bbox to valid image coordinates\"\"\"\n",
    "    xmin = tf.clip_by_value(xmin, 0, image_width)\n",
    "    ymin = tf.clip_by_value(ymin, 0, image_height)\n",
    "    xmax = tf.clip_by_value(xmax, 0, image_width)\n",
    "    ymax = tf.clip_by_value(ymax, 0, image_height)\n",
    "    return xmin, ymin, xmax, ymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _should_apply(prob):\n",
    "    \"\"\"Helper function to create bool tensor with probability\"\"\"\n",
    "    return tf.cast(tf.floor(tf.random_uniform([], dtype=tf.float32) + prob), tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset - COVIDxCTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COVIDxCTDataset:\n",
    "    \"\"\"COVIDx-CT dataset class, which handles construction of train/validation datasets\"\"\"\n",
    "    def __init__(self, data_dir, image_height=512, image_width=512, max_bbox_jitter=0.025, max_rotation=10,\n",
    "                 max_shear=0.15, max_pixel_shift=10, max_pixel_scale_change=0.2, shuffle_buffer=1000):\n",
    "        # General parameters\n",
    "        self.data_dir = data_dir\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.shuffle_buffer = shuffle_buffer\n",
    "\n",
    "        # Augmentation parameters\n",
    "        self.max_bbox_jitter = max_bbox_jitter\n",
    "        self.max_rotation = max_rotation\n",
    "        self.max_shear = max_shear\n",
    "        self.max_pixel_shift = max_pixel_shift\n",
    "        self.max_pixel_scale_change = max_pixel_scale_change\n",
    "\n",
    "    def train_dataset(self, train_split_file='train.txt', batch_size=1):\n",
    "        \"\"\"Returns training dataset\"\"\"\n",
    "        return self._make_dataset(train_split_file, batch_size, True)\n",
    "\n",
    "    def validation_dataset(self, val_split_file='val.txt', batch_size=1):\n",
    "        \"\"\"Returns validation dataset (also used for testing)\"\"\"\n",
    "        return self._make_dataset(val_split_file, batch_size, False)\n",
    "\n",
    "    def _make_dataset(self, split_file, batch_size, is_training, balanced=True):\n",
    "        \"\"\"Creates COVIDX-CT dataset for train or val split\"\"\"\n",
    "        files, classes, bboxes = self._get_files(split_file)\n",
    "        count = len(files)\n",
    "\n",
    "        # Create balanced dataset if required\n",
    "        if is_training and balanced:\n",
    "            files = np.asarray(files)\n",
    "            classes = np.asarray(classes, dtype=np.int32)\n",
    "            bboxes = np.asarray(bboxes, dtype=np.int32)\n",
    "            class_nums = np.unique(classes)\n",
    "            class_wise_datasets = []\n",
    "            for cls in class_nums:\n",
    "                indices = np.where(classes == cls)[0]\n",
    "                class_wise_datasets.append(\n",
    "                    tf.data.Dataset.from_tensor_slices((files[indices], classes[indices], bboxes[indices])))\n",
    "            class_weights = [1.0 / len(class_nums) for _ in class_nums]\n",
    "            dataset = tf.data.experimental.sample_from_datasets(\n",
    "                class_wise_datasets, class_weights)\n",
    "        else:\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((files, classes, bboxes))\n",
    "\n",
    "        # Shuffle and repeat in training\n",
    "        if is_training:\n",
    "            dataset = dataset.shuffle(buffer_size=self.shuffle_buffer)\n",
    "            dataset = dataset.repeat()\n",
    "\n",
    "        # Create and apply map function\n",
    "        load_and_process = self._get_load_and_process_fn(is_training)\n",
    "        dataset = dataset.map(load_and_process)\n",
    "\n",
    "        # Batch data\n",
    "        dataset = dataset.batch(batch_size)\n",
    "\n",
    "        return dataset, count, batch_size\n",
    "\n",
    "    def _get_load_and_process_fn(self, is_training):\n",
    "        \"\"\"Creates map function for TF dataset\"\"\"\n",
    "        def load_and_process(path, label, bbox):\n",
    "            # Load image\n",
    "            image = tf.image.decode_png(tf.io.read_file(path), channels=1)\n",
    "\n",
    "            # Apply augmentations and/or crop to bbox\n",
    "            if is_training:\n",
    "                image, bbox = self._augment_image_and_bbox(image, bbox)\n",
    "            else:\n",
    "                image = tf.image.crop_to_bounding_box(image, bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0])\n",
    "\n",
    "            # Stack to 3-channel, scale to [0, 1] and resize\n",
    "            image = tf.image.grayscale_to_rgb(image)\n",
    "            image = tf.cast(image, tf.float32)\n",
    "            image = image / 255.0\n",
    "            image = tf.image.resize(image, [self.image_height, self.image_width])\n",
    "            label = tf.cast(label, dtype=tf.int32)\n",
    "\n",
    "            return {'image': image, 'label': label}\n",
    "\n",
    "        return load_and_process\n",
    "\n",
    "    def _augment_image_and_bbox(self, image, bbox):\n",
    "        \"\"\"Apply augmentations to image and bbox\"\"\"\n",
    "        image_shape = tf.cast(tf.shape(image), tf.float32)\n",
    "        image_height, image_width = image_shape[0], image_shape[1]\n",
    "        bbox = random_bbox_jitter(bbox, image_height, image_width, self.max_bbox_jitter)\n",
    "        image, bbox = random_rotation(image, self.max_rotation, bbox)\n",
    "        image, bbox = random_shear(image, self.max_shear, bbox)\n",
    "        image = tf.image.crop_to_bounding_box(image, bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0])\n",
    "        image = random_shift_and_scale(image, self.max_pixel_shift, self.max_pixel_scale_change)\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        return image, bbox\n",
    "\n",
    "    def _get_files(self, split_file):\n",
    "        \"\"\"Gets image filenames and classes\"\"\"\n",
    "        files, classes, bboxes = [], [], []\n",
    "        with open(split_file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                fname, cls, xmin, ymin, xmax, ymax = line.strip('\\n').split()\n",
    "                files.append(os.path.join(self.data_dir, fname))\n",
    "                classes.append(int(cls))\n",
    "                bboxes.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
    "        return files, classes, bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!python run_covidnet_ct.py train     --model_dir models/COVIDNet-CT-A     --meta_name model.meta     --ckpt_name model'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''!python run_covidnet_ct.py train \\\n",
    "    --model_dir models/COVIDNet-CT-A \\\n",
    "    --meta_name model.meta \\\n",
    "    --ckpt_name model'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_covidnet_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict keys\n",
    "TRAIN_OP_KEY = 'train_op'\n",
    "TF_SUMMARY_KEY = 'tf_summaries'\n",
    "LOSS_KEY = 'loss'\n",
    "\n",
    "# Tensor names\n",
    "IMAGE_INPUT_TENSOR = 'Placeholder:0'\n",
    "LABEL_INPUT_TENSOR = 'Placeholder_1:0'\n",
    "CLASS_PRED_TENSOR = 'ArgMax:0'\n",
    "CLASS_PROB_TENSOR = 'softmax_tensor:0'\n",
    "TRAINING_PH_TENSOR = 'is_training:0'\n",
    "LOSS_TENSOR = 'add:0'\n",
    "\n",
    "# Names for train checkpoints\n",
    "CKPT_NAME = 'model.ckpt'\n",
    "MODEL_NAME = 'COVIDNet-CT'\n",
    "'output'\n",
    "# Output directory for storing runs\n",
    "OUTPUT_DIR = 'output'\n",
    "\n",
    "# Class names ordered by class index\n",
    "CLASS_NAMES = ('Normal', 'Pneumonia', 'COVID-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_grad_filter(gvs):\n",
    "    \"\"\"Filter to apply gradient updates to dense layers only\"\"\"\n",
    "    return [(g, v) for g, v in gvs if 'dense' in v.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_summary(tag_to_value, tag_prefix=''):\n",
    "    \"\"\"Summary object for a dict of python scalars\"\"\"\n",
    "    return tf.Summary(value=[tf.Summary.Value(tag=tag_prefix + tag, simple_value=value)\n",
    "                             for tag, value in tag_to_value.items() if isinstance(value, (int, float))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session():\n",
    "    \"\"\"Helper function for session creation\"\"\"\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "class Metrics:\n",
    "    \"\"\"Lightweight class for tracking metrics\"\"\"\n",
    "    def __init__(self):\n",
    "        num_classes = len(CLASS_NAMES)\n",
    "        self.labels = list(range(num_classes))\n",
    "        self.class_names = CLASS_NAMES\n",
    "        self.confusion_matrix = np.zeros((num_classes, num_classes), dtype=np.uint32)\n",
    "\n",
    "    def update(self, y_true, y_pred):\n",
    "        self.confusion_matrix = self.confusion_matrix + confusion_matrix(y_true, y_pred, labels=self.labels)\n",
    "\n",
    "    def reset(self):\n",
    "        self.confusion_matrix *= 0\n",
    "\n",
    "    def values(self):\n",
    "        conf_matrix = self.confusion_matrix.astype('float')\n",
    "        metrics = {\n",
    "            'accuracy': np.diag(conf_matrix).sum() / conf_matrix.sum(),\n",
    "            'confusion matrix': self.confusion_matrix.copy()\n",
    "        }\n",
    "        sensitivity = np.diag(conf_matrix) / np.maximum(conf_matrix.sum(axis=1), 1)\n",
    "        pos_pred_val = np.diag(conf_matrix) / np.maximum(conf_matrix.sum(axis=0), 1)\n",
    "        for cls, idx, sens, ppv in zip(self.class_names, self.labels, sensitivity, pos_pred_val):\n",
    "            metrics['{} {}'.format(cls, 'sensitivity')] = sensitivity[idx]\n",
    "            metrics['{} {}'.format(cls, 'PPV')] = pos_pred_val[idx]\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COVIDNetCTRunner:\n",
    "    \"\"\"Primary training/testing/inference class\"\"\"\n",
    "    def __init__(self, meta_file, ckpt=None, data_dir=None, input_height=224, input_width=224, max_bbox_jitter=0.025,\n",
    "                 max_rotation=10, max_shear=0.15, max_pixel_shift=10, max_pixel_scale_change=0.2):\n",
    "        self.meta_file = meta_file\n",
    "        self.ckpt = ckpt\n",
    "        self.input_height = input_height\n",
    "        self.input_width = input_width\n",
    "        if data_dir is None:\n",
    "            self.dataset = None\n",
    "        else:\n",
    "            self.dataset = COVIDxCTDataset(\n",
    "                data_dir,\n",
    "                image_height=input_height,\n",
    "                image_width=input_width,\n",
    "                max_bbox_jitter=max_bbox_jitter,\n",
    "                max_rotation=max_rotation,\n",
    "                max_shear=max_shear,\n",
    "                max_pixel_shift=max_pixel_shift,\n",
    "                max_pixel_scale_change=max_pixel_scale_change\n",
    "            )\n",
    "\n",
    "    def load_graph(self):\n",
    "        \"\"\"Creates new graph and session\"\"\"\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            # Create session and load model\n",
    "            sess = create_session()\n",
    "\n",
    "            # Load meta file\n",
    "            print('Loading meta graph from ' + self.meta_file)\n",
    "            saver = tf.train.import_meta_graph(self.meta_file)\n",
    "        return graph, sess, saver\n",
    "\n",
    "    def load_ckpt(self, sess, saver):\n",
    "        \"\"\"Helper for loading weights\"\"\"\n",
    "        # Load weights\n",
    "        if self.ckpt is not None:\n",
    "            print('Loading weights from ' + self.ckpt)\n",
    "            saver.restore(sess, self.ckpt)\n",
    "\n",
    "    def trainval(self, epochs, output_dir, batch_size=1, learning_rate=0.001, momentum=0.9,\n",
    "                 fc_only=False, train_split_file='train.txt', val_split_file='val.txt',\n",
    "                 log_interval=20, val_interval=1000, save_interval=1000):\n",
    "        \"\"\"Run training with intermittent validation\"\"\"\n",
    "        ckpt_path = os.path.join(output_dir, CKPT_NAME)\n",
    "        graph, sess, saver = self.load_graph()\n",
    "        with graph.as_default():\n",
    "            # Create optimizer\n",
    "            optimizer = tf.train.MomentumOptimizer(\n",
    "                learning_rate=learning_rate,\n",
    "                momentum=momentum\n",
    "            )\n",
    "\n",
    "            # Create train op\n",
    "            global_step = tf.train.get_or_create_global_step()\n",
    "            loss = graph.get_tensor_by_name(LOSS_TENSOR)\n",
    "            grad_vars = optimizer.compute_gradients(loss)\n",
    "            if fc_only:\n",
    "                grad_vars = dense_grad_filter(grad_vars)\n",
    "            minimize_op = optimizer.apply_gradients(grad_vars, global_step)\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            train_op = tf.group(minimize_op, update_ops)\n",
    "\n",
    "            # Load checkpoint\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            self.load_ckpt(sess, saver)\n",
    "\n",
    "            # Create train dataset\n",
    "            dataset, num_images, batch_size = self.dataset.train_dataset(train_split_file, batch_size)\n",
    "            data_next = dataset.make_one_shot_iterator().get_next()\n",
    "            num_iters = ceil(num_images / batch_size) * epochs\n",
    "\n",
    "            # Create feed and fetch dicts\n",
    "            feed_dict = {TRAINING_PH_TENSOR: True}\n",
    "            fetch_dict = {\n",
    "                TRAIN_OP_KEY: train_op,\n",
    "                LOSS_KEY: LOSS_TENSOR\n",
    "            }\n",
    "\n",
    "            # Add summaries\n",
    "            summary_writer = tf.summary.FileWriter(os.path.join(output_dir, 'events'), graph)\n",
    "            fetch_dict[TF_SUMMARY_KEY] = self._get_train_summary_op(graph)\n",
    "\n",
    "            # Create validation function\n",
    "            run_validation = self._get_validation_fn(sess, batch_size, val_split_file)\n",
    "\n",
    "            # Baseline saving and validation\n",
    "            print('Saving baseline checkpoint')\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, ckpt_path, global_step=0)\n",
    "            print('Starting baseline validation')\n",
    "            metrics = run_validation()\n",
    "            self._log_and_print_metrics(metrics, 0, summary_writer)\n",
    "\n",
    "            # Training loop\n",
    "            print('Training with batch_size {} for {} steps'.format(batch_size, num_iters))\n",
    "            for i in range(num_iters):\n",
    "                # Run training step\n",
    "                data = sess.run(data_next)\n",
    "                feed_dict[IMAGE_INPUT_TENSOR] = data['image']\n",
    "                feed_dict[LABEL_INPUT_TENSOR] = data['label']\n",
    "                results = sess.run(fetch_dict, feed_dict)\n",
    "\n",
    "                # Log and save\n",
    "                step = i + 1\n",
    "                if step % log_interval == 0:\n",
    "                    summary_writer.add_summary(results[TF_SUMMARY_KEY], step)\n",
    "                    print('[step: {}, loss: {}]'.format(step, results[LOSS_KEY]))\n",
    "                if step % save_interval == 0:\n",
    "                    print('Saving checkpoint at step {}'.format(step))\n",
    "                    saver.save(sess, ckpt_path, global_step=step)\n",
    "                if val_interval > 0 and step % val_interval == 0:\n",
    "                    print('Starting validation at step {}'.format(step))\n",
    "                    metrics = run_validation()\n",
    "                    self._log_and_print_metrics(metrics, step, summary_writer)\n",
    "\n",
    "            print('Saving checkpoint at last step')\n",
    "            saver.save(sess, ckpt_path, global_step=num_iters)\n",
    "\n",
    "    def test(self, batch_size=1, test_split_file='test.txt', plot_confusion=False):\n",
    "        \"\"\"Run test on a checkpoint\"\"\"\n",
    "        graph, sess, saver = self.load_graph()\n",
    "        with graph.as_default():\n",
    "            # Load checkpoint\n",
    "            self.load_ckpt(sess, saver)\n",
    "\n",
    "            # Run test\n",
    "            print('Starting test')\n",
    "            metrics = self._get_validation_fn(sess, batch_size, test_split_file)()\n",
    "            self._log_and_print_metrics(metrics)\n",
    "\n",
    "            if plot_confusion:\n",
    "                # Plot confusion matrix\n",
    "                fig, ax = plt.subplots()\n",
    "                disp = ConfusionMatrixDisplay(confusion_matrix=metrics['confusion matrix'],\n",
    "                                              display_labels=CLASS_NAMES)\n",
    "                disp.plot(include_values=True, cmap='Blues', ax=ax, xticks_rotation='horizontal', values_format='.5g')\n",
    "                plt.show()\n",
    "\n",
    "    def infer(self, image_file, autocrop=False):\n",
    "        \"\"\"Run inference on the given image\"\"\"\n",
    "        # Load and preprocess image\n",
    "        image = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE)\n",
    "        if autocrop:\n",
    "            image, _ = auto_body_crop(image)\n",
    "        image = cv2.resize(image, (self.input_width, self.input_height), cv2.INTER_CUBIC)\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = np.expand_dims(np.stack((image, image, image), axis=-1), axis=0)\n",
    "\n",
    "        # Create feed dict\n",
    "        feed_dict = {IMAGE_INPUT_TENSOR: image, TRAINING_PH_TENSOR: False}\n",
    "\n",
    "        # Run inference\n",
    "        graph, sess, saver = self.load_graph()\n",
    "        with graph.as_default():\n",
    "            # Load checkpoint\n",
    "            self.load_ckpt(sess, saver)\n",
    "\n",
    "            # Run image through model\n",
    "            class_, probs = sess.run([CLASS_PRED_TENSOR, CLASS_PROB_TENSOR], feed_dict=feed_dict)\n",
    "            print('\\nPredicted Class: ' + CLASS_NAMES[class_[0]])\n",
    "            print('Confidences:' + ', '.join(\n",
    "                '{}: {}'.format(name, conf) for name, conf in zip(CLASS_NAMES, probs[0])))\n",
    "            print('**DISCLAIMER**')\n",
    "            print('Do not use this prediction for self-diagnosis. '\n",
    "                  'You should check with your local authorities for '\n",
    "                  'the latest advice on seeking medical assistance.')\n",
    "\n",
    "    def _get_validation_fn(self, sess, batch_size=1, val_split_file='val.txt'):\n",
    "        \"\"\"Creates validation function to call in self.trainval() or self.test()\"\"\"\n",
    "        # Create val dataset\n",
    "        dataset, num_images, batch_size = self.dataset.validation_dataset(val_split_file, batch_size)\n",
    "        dataset = dataset.repeat()  # repeat so there is no need to reconstruct it\n",
    "        data_next = dataset.make_one_shot_iterator().get_next()\n",
    "        num_iters = ceil(num_images / batch_size)\n",
    "\n",
    "        # Create running accuracy metric\n",
    "        metrics = Metrics()\n",
    "\n",
    "        # Create feed and fetch dicts\n",
    "        fetch_dict = {'classes': CLASS_PRED_TENSOR}\n",
    "        feed_dict = {TRAINING_PH_TENSOR: False}\n",
    "\n",
    "        def run_validation():\n",
    "            metrics.reset()\n",
    "            for i in range(num_iters):\n",
    "                data = sess.run(data_next)\n",
    "                feed_dict[IMAGE_INPUT_TENSOR] = data['image']\n",
    "                results = sess.run(fetch_dict, feed_dict)\n",
    "                metrics.update(data['label'], results['classes'])\n",
    "            return metrics.values()\n",
    "\n",
    "        return run_validation\n",
    "\n",
    "    @staticmethod\n",
    "    def _log_and_print_metrics(metrics, step=None, summary_writer=None, tag_prefix='val/'):\n",
    "        \"\"\"Helper for logging and printing\"\"\"\n",
    "        # Pop temporarily and print\n",
    "        cm = metrics.pop('confusion matrix')\n",
    "        print('\\tconfusion matrix:')\n",
    "        print('\\t' + str(cm).replace('\\n', '\\n\\t'))\n",
    "\n",
    "        # Print scalar metrics\n",
    "        for name, val in sorted(metrics.items()):\n",
    "            print('\\t{}: {}'.format(name, val))\n",
    "\n",
    "        # Log scalar metrics\n",
    "        if summary_writer is not None:\n",
    "            summary = simple_summary(metrics, tag_prefix)\n",
    "            summary_writer.add_summary(summary, step)\n",
    "\n",
    "        # Restore confusion matrix\n",
    "        metrics['confusion matrix'] = cm\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_train_summary_op(graph, tag_prefix='train/'):\n",
    "        loss = graph.get_tensor_by_name(LOSS_TENSOR)\n",
    "        loss_summary = tf.summary.scalar(tag_prefix + 'loss', loss)\n",
    "        return loss_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress most console output\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta graph from models/COVIDNet-CT-A/model.meta\n",
      "Loading weights from models/COVIDNet-CT-A/model\n",
      "Saving baseline checkpoint\n",
      "Starting baseline validation\n",
      "\tconfusion matrix:\n",
      "\t[[ 0  0  0]\n",
      "\t [ 0 49  0]\n",
      "\t [ 0  1 82]]\n",
      "\tCOVID-19 PPV: 1.0\n",
      "\tCOVID-19 sensitivity: 0.9879518072289156\n",
      "\tNormal PPV: 0.0\n",
      "\tNormal sensitivity: 0.0\n",
      "\tPneumonia PPV: 0.98\n",
      "\tPneumonia sensitivity: 1.0\n",
      "\taccuracy: 0.9924242424242424\n",
      "Training with batch_size 8 for 1420 steps\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-fc77204d6855>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mlog_interval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mval_interval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_interval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0msave_interval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     )\n\u001b[0;32m     78\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-257fb621228b>\u001b[0m in \u001b[0;36mtrainval\u001b[1;34m(self, epochs, output_dir, batch_size, learning_rate, momentum, fc_only, train_split_file, val_split_file, log_interval, val_interval, save_interval)\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mIMAGE_INPUT_TENSOR\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mLABEL_INPUT_TENSOR\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                 \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[1;31m# Log and save\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# mode = 'train' \n",
    "#args = ['models/COVIDNet-CT-A', 'model.meta', 'model']  ## model_dir, meta_name, ckpt_name\n",
    "\n",
    "# Create full paths\n",
    "model_dir = 'models/COVIDNet-CT-A'\n",
    "meta_file = 'models/COVIDNet-CT-A/model.meta'\n",
    "ckpt = 'models/COVIDNet-CT-A/model'\n",
    "input_height = 512\n",
    "input_width = 512\n",
    "\n",
    "# Create runner\n",
    "if mode == 'train':\n",
    "    augmentation_kwargs = dict(\n",
    "        max_bbox_jitter = 0.075,\n",
    "        max_rotation = 15,\n",
    "        max_shear = 0.2,\n",
    "        max_pixel_shift = 15,\n",
    "        max_pixel_scale_change = 0.15\n",
    "    )\n",
    "else:\n",
    "    augmentation_kwargs = {}\n",
    "        \n",
    "runner = COVIDNetCTRunner(\n",
    "    meta_file,\n",
    "    ckpt = ckpt,\n",
    "    data_dir = 'data/COVIDx-CT',\n",
    "    input_height = input_height,\n",
    "    input_width = input_width,\n",
    "    max_bbox_jitter = 0.075,\n",
    "    max_rotation = 15,\n",
    "    max_shear = 0.2,\n",
    "    max_pixel_shift = 15,\n",
    "    max_pixel_scale_change = 0.15\n",
    ")\n",
    "\n",
    "if mode == 'train':\n",
    "    output_suffix = datetime.now().strftime('_%Y-%m-%d_%H.%M.%S')\n",
    "    # Create output_dir and save run settings\n",
    "    output_dir = os.path.join(OUTPUT_DIR, MODEL_NAME + output_suffix)\n",
    "    os.makedirs(output_dir, exist_ok=False)\n",
    "    \n",
    "    data_dir = 'data/COVIDx-CT'\n",
    "    train_split_file = 'train_COVIDx-CT.txt'\n",
    "    val_split_file = 'val_COVIDx-CT.txt'\n",
    "    epochs = 20\n",
    "    batch_size = 8\n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.9\n",
    "    fc_only = 'store_true'\n",
    "    log_interval = 50\n",
    "    val_interval = 2000\n",
    "    save_interval = 2000\n",
    "    \n",
    "    args = {\"model_dir\": \"models/COVIDNet-CT-A\", \"meta_name\": \"model.meta\", \"ckpt_name\": \"model\", \"input_height\": 512, \\\n",
    "     \"input_width\": 512, \"output_suffix\": output_suffix, \"data_dir\": \"data/COVIDx-CT\", \\\n",
    "     \"train_split_file\": \"train_COVIDx-CT.txt\", \"val_split_file\": \"val_COVIDx-CT.txt\", \"epochs\": 20, \"batch_size\": 8,\\\n",
    "     \"learning_rate\": 0.001, \"momentum\": 0.9, \"fc_only\": fc_only, \"log_interval\": 50, \"val_interval\": 2000,\\\n",
    "     \"save_interval\": 2000, \"max_bbox_jitter\": 0.075, \"max_rotation\": 15, \"max_shear\": 0.2, \\\n",
    "     \"max_pixel_shift\": 15, \"max_pixel_scale_change\": 0.15}\n",
    "    \n",
    "    '''with open(os.path.join(output_dir, 'run_settings.json'), 'w') as f:\n",
    "        json.dump(vars(args), f)'''\n",
    "\n",
    "    # Run trainval\n",
    "    runner.trainval(\n",
    "        epochs,\n",
    "        output_dir,\n",
    "        batch_size = batch_size,\n",
    "        learning_rate = learning_rate,\n",
    "        momentum = momentum,\n",
    "        fc_only = fc_only,\n",
    "        train_split_file = train_split_file,\n",
    "        val_split_file = val_split_file,\n",
    "        log_interval = log_interval,\n",
    "        val_interval = val_interval,\n",
    "        save_interval = save_interval\n",
    "    )\n",
    "elif mode == 'test':\n",
    "    data_dir = 'data/COVIDx-CT'\n",
    "    batch_size = 8\n",
    "    test_split_file = 'test_COVIDx-CT.txt'\n",
    "    plot_confusion = 'store_true'\n",
    "    \n",
    "    # Run validation\n",
    "    runner.test(\n",
    "        batch_size = batch_size,\n",
    "        test_split_file = test_split_file,\n",
    "        plot_confusion = plot_confusion\n",
    "    )\n",
    "elif mode == 'infer':\n",
    "    image_file = 'assets/ex-covid-ct.png'\n",
    "    auto_crop = 'store_true'\n",
    "    # Run inference\n",
    "    runner.infer(image_file, auto_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode = 'train' \n",
    "#args = ['models/COVIDNet-CT-A', 'model.meta', 'model']  ## model_dir, meta_name, ckpt_name\n",
    "\n",
    "# Create full paths\n",
    "model_dir = 'models/COVIDNet-CT-A'\n",
    "meta_file = 'models/COVIDNet-CT-A/model.meta'\n",
    "ckpt = 'models/COVIDNet-CT-A/model'\n",
    "input_height = 512\n",
    "input_width = 512\n",
    "\n",
    "# Create runner\n",
    "if mode == 'train':\n",
    "    augmentation_kwargs = dict(\n",
    "        max_bbox_jitter = 0.075,\n",
    "        max_rotation = 15,\n",
    "        max_shear = 0.2,\n",
    "        max_pixel_shift = 15,\n",
    "        max_pixel_scale_change = 0.15\n",
    "    )\n",
    "else:\n",
    "    augmentation_kwargs = {}\n",
    "        \n",
    "runner = COVIDNetCTRunner(\n",
    "    meta_file,\n",
    "    ckpt = ckpt,\n",
    "    data_dir = 'data/COVIDx-CT',\n",
    "    input_height = input_height,\n",
    "    input_width = input_width,\n",
    "    max_bbox_jitter = 0.075,\n",
    "    max_rotation = 15,\n",
    "    max_shear = 0.2,\n",
    "    max_pixel_shift = 15,\n",
    "    max_pixel_scale_change = 0.15\n",
    ")\n",
    "\n",
    "if mode == 'train':\n",
    "    output_suffix = datetime.now().strftime('_%Y-%m-%d_%H.%M.%S')\n",
    "    # Create output_dir and save run settings\n",
    "    output_dir = os.path.join(OUTPUT_DIR, MODEL_NAME + output_suffix)\n",
    "    os.makedirs(output_dir, exist_ok=False)\n",
    "    \n",
    "    data_dir = 'data/COVIDx-CT'\n",
    "    train_split_file = 'train_COVIDx-CT.txt'\n",
    "    val_split_file = 'val_COVIDx-CT.txt'\n",
    "    epochs = 20\n",
    "    batch_size = 8\n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.9\n",
    "    fc_only = 'store_true'\n",
    "    log_interval = 50\n",
    "    val_interval = 2000\n",
    "    save_interval = 2000\n",
    "    \n",
    "    args = {\"model_dir\": \"models/COVIDNet-CT-A\", \"meta_name\": \"model.meta\", \"ckpt_name\": \"model\", \"input_height\": 512, \\\n",
    "     \"input_width\": 512, \"output_suffix\": output_suffix, \"data_dir\": \"data/COVIDx-CT\", \\\n",
    "     \"train_split_file\": \"train_COVIDx-CT.txt\", \"val_split_file\": \"val_COVIDx-CT.txt\", \"epochs\": 20, \"batch_size\": 8,\\\n",
    "     \"learning_rate\": 0.001, \"momentum\": 0.9, \"fc_only\": fc_only, \"log_interval\": 50, \"val_interval\": 2000,\\\n",
    "     \"save_interval\": 2000, \"max_bbox_jitter\": 0.075, \"max_rotation\": 15, \"max_shear\": 0.2, \\\n",
    "     \"max_pixel_shift\": 15, \"max_pixel_scale_change\": 0.15}\n",
    "    \n",
    "    '''with open(os.path.join(output_dir, 'run_settings.json'), 'w') as f:\n",
    "        json.dump(vars(args), f)'''\n",
    "\n",
    "    # Run trainval\n",
    "    runner.trainval(\n",
    "        epochs,\n",
    "        output_dir,\n",
    "        batch_size = batch_size,\n",
    "        learning_rate = learning_rate,\n",
    "        momentum = momentum,\n",
    "        fc_only = fc_only,\n",
    "        train_split_file = train_split_file,\n",
    "        val_split_file = val_split_file,\n",
    "        log_interval = log_interval,\n",
    "        val_interval = val_interval,\n",
    "        save_interval = save_interval\n",
    "    )\n",
    "elif mode == 'test':\n",
    "    data_dir = 'data/COVIDx-CT'\n",
    "    batch_size = 8\n",
    "    test_split_file = 'test_COVIDx-CT.txt'\n",
    "    plot_confusion = 'store_true'\n",
    "    \n",
    "    # Run validation\n",
    "    runner.test(\n",
    "        batch_size = batch_size,\n",
    "        test_split_file = test_split_file,\n",
    "        plot_confusion = plot_confusion\n",
    "    )\n",
    "elif mode == 'infer':\n",
    "    image_file = 'assets/ex-covid-ct.png'\n",
    "    auto_crop = 'store_true'\n",
    "    # Run inference\n",
    "    runner.infer(image_file, auto_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'infer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode = 'train' \n",
    "#args = ['models/COVIDNet-CT-A', 'model.meta', 'model']  ## model_dir, meta_name, ckpt_name\n",
    "\n",
    "# Create full paths\n",
    "model_dir = 'models/COVIDNet-CT-A'\n",
    "meta_file = 'models/COVIDNet-CT-A/model.meta'\n",
    "ckpt = 'models/COVIDNet-CT-A/model'\n",
    "input_height = 512\n",
    "input_width = 512\n",
    "\n",
    "# Create runner\n",
    "if mode == 'train':\n",
    "    augmentation_kwargs = dict(\n",
    "        max_bbox_jitter = 0.075,\n",
    "        max_rotation = 15,\n",
    "        max_shear = 0.2,\n",
    "        max_pixel_shift = 15,\n",
    "        max_pixel_scale_change = 0.15\n",
    "    )\n",
    "else:\n",
    "    augmentation_kwargs = {}\n",
    "        \n",
    "runner = COVIDNetCTRunner(\n",
    "    meta_file,\n",
    "    ckpt = ckpt,\n",
    "    data_dir = 'data/COVIDx-CT',\n",
    "    input_height = input_height,\n",
    "    input_width = input_width,\n",
    "    max_bbox_jitter = 0.075,\n",
    "    max_rotation = 15,\n",
    "    max_shear = 0.2,\n",
    "    max_pixel_shift = 15,\n",
    "    max_pixel_scale_change = 0.15\n",
    ")\n",
    "\n",
    "if mode == 'train':\n",
    "    output_suffix = datetime.now().strftime('_%Y-%m-%d_%H.%M.%S')\n",
    "    # Create output_dir and save run settings\n",
    "    output_dir = os.path.join(OUTPUT_DIR, MODEL_NAME + output_suffix)\n",
    "    os.makedirs(output_dir, exist_ok=False)\n",
    "    \n",
    "    data_dir = 'data/COVIDx-CT'\n",
    "    train_split_file = 'train_COVIDx-CT.txt'\n",
    "    val_split_file = 'val_COVIDx-CT.txt'\n",
    "    epochs = 20\n",
    "    batch_size = 8\n",
    "    learning_rate = 0.001\n",
    "    momentum = 0.9\n",
    "    fc_only = 'store_true'\n",
    "    log_interval = 50\n",
    "    val_interval = 2000\n",
    "    save_interval = 2000\n",
    "    \n",
    "    args = {\"model_dir\": \"models/COVIDNet-CT-A\", \"meta_name\": \"model.meta\", \"ckpt_name\": \"model\", \"input_height\": 512, \\\n",
    "     \"input_width\": 512, \"output_suffix\": output_suffix, \"data_dir\": \"data/COVIDx-CT\", \\\n",
    "     \"train_split_file\": \"train_COVIDx-CT.txt\", \"val_split_file\": \"val_COVIDx-CT.txt\", \"epochs\": 20, \"batch_size\": 8,\\\n",
    "     \"learning_rate\": 0.001, \"momentum\": 0.9, \"fc_only\": fc_only, \"log_interval\": 50, \"val_interval\": 2000,\\\n",
    "     \"save_interval\": 2000, \"max_bbox_jitter\": 0.075, \"max_rotation\": 15, \"max_shear\": 0.2, \\\n",
    "     \"max_pixel_shift\": 15, \"max_pixel_scale_change\": 0.15}\n",
    "    \n",
    "    '''with open(os.path.join(output_dir, 'run_settings.json'), 'w') as f:\n",
    "        json.dump(vars(args), f)'''\n",
    "\n",
    "    # Run trainval\n",
    "    runner.trainval(\n",
    "        epochs,\n",
    "        output_dir,\n",
    "        batch_size = batch_size,\n",
    "        learning_rate = learning_rate,\n",
    "        momentum = momentum,\n",
    "        fc_only = fc_only,\n",
    "        train_split_file = train_split_file,\n",
    "        val_split_file = val_split_file,\n",
    "        log_interval = log_interval,\n",
    "        val_interval = val_interval,\n",
    "        save_interval = save_interval\n",
    "    )\n",
    "elif mode == 'test':\n",
    "    data_dir = 'data/COVIDx-CT'\n",
    "    batch_size = 8\n",
    "    test_split_file = 'test_COVIDx-CT.txt'\n",
    "    plot_confusion = 'store_true'\n",
    "    \n",
    "    # Run validation\n",
    "    runner.test(\n",
    "        batch_size = batch_size,\n",
    "        test_split_file = test_split_file,\n",
    "        plot_confusion = plot_confusion\n",
    "    )\n",
    "elif mode == 'infer':\n",
    "    image_file = 'assets/ex-covid-ct.png'\n",
    "    auto_crop = 'store_true'\n",
    "    # Run inference\n",
    "    runner.infer(image_file, auto_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''!python run_covidnet_ct.py train \\\n",
    "    --model_dir models/COVIDNet-CT-A \\\n",
    "    --meta_name model.meta \\\n",
    "    --ckpt_name model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''!python run_covidnet_ct.py test \\\n",
    "    --model_dir models/COVIDNet-CT-A \\\n",
    "    --meta_name model.meta \\\n",
    "    --ckpt_name model \\\n",
    "    --plot_confusion'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''!python run_covidnet_ct.py infer \\\n",
    "    --model_dir models/COVIDNet-CT-A \\\n",
    "    --meta_name model.meta \\\n",
    "    --ckpt_name model \\\n",
    "    --image_file assets/ex-covid-ct.png'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def parse_args(args):\n",
    "    \"\"\"Argument parsing for run_covidnet_ct.py\"\"\"\n",
    "    model_dir = 'models/COVIDNet-CT-A'\n",
    "    meta_name = 'model.meta'\n",
    "    ckpt_name = 'model'\n",
    "    input_height = 512\n",
    "    input_width = 512\n",
    "    if args[0] == 'train':\n",
    "        # General training parameters\n",
    "        output_suffix = datetime.now().strftime('_%Y-%m-%d_%H.%M.%S')\n",
    "        data_dir = 'data/COVIDx-CT'\n",
    "        train_split_file = 'train_COVIDx-CT.txt'\n",
    "        val_split_file = 'val_COVIDx-CT.txt'\n",
    "        epochs = 20\n",
    "        batch_size = 8\n",
    "        learning_rate = 0.001\n",
    "        momentum = 0.9\n",
    "        fc_only = 'store_true'\n",
    "        log_interval = 50\n",
    "        val_interval = 2000\n",
    "        save_interval = 2000\n",
    "        \n",
    "        # Augmentation parameters\n",
    "        max_bbox_jitter = 0.075\n",
    "        max_rotation = 15\n",
    "        max_shear = 0.2\n",
    "        max_pixel_shift = 15\n",
    "        max_pixel_scale_change = 0.15\n",
    "        \n",
    "    elif args[0] == 'test':\n",
    "        data_dir = 'data/COVIDx-CT'\n",
    "        batch_size = 8\n",
    "        test_split_file = 'test_COVIDx-CT.txt'\n",
    "        plot_confusion = 'store_true'\n",
    "        \n",
    "    elif args[0] == 'infer':\n",
    "        image_file = 'assets/ex-covid-ct.png'\n",
    "        auto_crop = 'store_true'\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Mode must be one of {train, test, infer} or {-h, --help}')\n",
    "\n",
    "    parsed_args = parser.parse_args(args[1:])\n",
    "    if args[0] == 'infer':\n",
    "        parsed_args.data_dir = None\n",
    "\n",
    "    return args[0], parsed_args\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    args = {model_dir : \"models/COVIDNet-CT-A\",\n",
    "            meta_name : \"model.meta\",\n",
    "            ckpt_name : \"model\",\n",
    "            input_height : 512,\n",
    "            input_width : 512,\n",
    "            output_suffix : output_suffix,\n",
    "            data_dir: data_dir,\n",
    "            train_split_file : train_split_file,\n",
    "            val_split_file = val_split_file,\n",
    "            epochs = epochs,\n",
    "            batch_size = batch_size,\n",
    "            learning_rate = learning_rate,\n",
    "            momentum = momentum, \n",
    "            fc_only = fc_only, \n",
    "            log_interval = log_interval, \n",
    "            val_interval = val_interval, \n",
    "            save_interval = save_interval, \n",
    "            max_bbox_jitter = 0.075,\n",
    "            max_rotation = 15,\n",
    "            max_shear = 0.2,\n",
    "            max_pixel_shift = 15,\n",
    "            max_pixel_scale_change = 0.15}'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
